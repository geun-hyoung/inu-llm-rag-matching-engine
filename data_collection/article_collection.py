"""
EBSCO ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘ê¸°
MariaDBì˜ v_emp1_3 í…Œì´ë¸”ì—ì„œ ë…¼ë¬¸ ì •ë³´ë¥¼ ê°€ì ¸ì™€
EBSCOì—ì„œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import mariadb
import pandas as pd
from playwright.sync_api import sync_playwright
import time
import json
from pathlib import Path
import sys

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
from config.database import get_db_connection, close_db_connection, get_article_data, COL_ARTICLE_EMP_NO, COL_ARTICLE_THSS_NM, COL_ARTICLE_PUBLSH_DT
from config.settings import ARTICLE_DATA_FILE

# DB ì—°ê²° ë° ë°ì´í„° ì¡°íšŒ
conn = None
try:
    conn = get_db_connection()
    
    # ë…¼ë¬¸ ë°ì´í„° ì¡°íšŒ (2015ë…„ ì´ìƒ)
    print("\nğŸ“š ë…¼ë¬¸ ë°ì´í„° ì¡°íšŒ ì¤‘...")
    df_emp = get_article_data(conn, min_year=2015)
    
    print(f"2015ë…„ ì´ìƒ ë…¼ë¬¸ í•„í„°ë§ í›„: {len(df_emp)}ê°œ")
    print(f"ì¤‘ë³µ ì œê±° í›„: {len(df_emp)}ê°œ")
    print(f"ê²Œì¬ì¼ì ìˆœ ì •ë ¬ ì™„ë£Œ (ìµœì‹ ìˆœ)")
    
    # í™•ì¸
    print("\nì´ (EMP_NO, THSS_NM) ì„¸íŠ¸ ìˆ˜:", len(df_emp))
    print("\n[ë¯¸ë¦¬ë³´ê¸° TOP 10 - ìµœì‹ ìˆœ]")
    print(df_emp[[COL_ARTICLE_EMP_NO, COL_ARTICLE_THSS_NM, COL_ARTICLE_PUBLSH_DT]].head(10))

except mariadb.Error as e:
    print("MariaDB ì—°ê²° ì‹¤íŒ¨!")
    print("ì˜¤ë¥˜ ì½”ë“œ:", e.errno)
    print("ì˜¤ë¥˜ ë©”ì‹œì§€:", e.msg)
    sys.exit(1)

EBSCO_URL = "https://research.ebsco.com/c/4zvbuh/search"

# df_emp ì— THSS_NMê³¼ EMP_NOë¥¼ í•¨ê»˜ ì‚¬ìš©
# (EMP_NO, THSS_NM) ìŒìœ¼ë¡œ ì €ì¥ (THSS_NM ì¤‘ë³µ ê°€ëŠ¥í•˜ì§€ë§Œ EMP_NOëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
queries_list = []
for _, row in df_emp.iterrows():
    queries_list.append({
        "EMP_NO": str(row[COL_ARTICLE_EMP_NO]).strip(),
        "THSS_NM": str(row[COL_ARTICLE_THSS_NM]).strip()
    })

# THSS_NM ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ê°™ì€ ì œëª©ì´ ì—¬ëŸ¬ EMP_NOì— ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ ê²ƒë§Œ ì‚¬ìš©)
seen_titles = set()
unique_queries = []
for q_dict in queries_list:
    if q_dict["THSS_NM"] not in seen_titles and q_dict["THSS_NM"]:
        seen_titles.add(q_dict["THSS_NM"])
        unique_queries.append(q_dict)

queries = unique_queries
total_queries = len(queries)
print(f"\nì´ {total_queries}ê°œì˜ ë…¼ë¬¸ ì œëª©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

# ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ (ì¤‘ê°„ ì €ì¥ìš©)
output_file = Path(ARTICLE_DATA_FILE)
output_file.parent.mkdir(parents=True, exist_ok=True)
existing_results = []
processed_titles = set()

try:
    with open(output_file, 'r', encoding='utf-8') as f:
        existing_results = json.load(f)
        processed_titles = set([str(item.get('THSS_NM', '')) for item in existing_results if item.get('THSS_NM')])
        print(f"ê¸°ì¡´ íŒŒì¼ì—ì„œ {len(existing_results)}ê°œì˜ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        print(f"ì´ë¯¸ ì²˜ë¦¬ëœ ë…¼ë¬¸: {len(processed_titles)}ê°œ")
        # ê¸°ì¡´ íŒŒì¼ì— EMP_NOê°€ ì—†ìœ¼ë©´ ì¶”ê°€ (í˜¸í™˜ì„±ì„ ìœ„í•´)
        if existing_results and 'EMP_NO' not in existing_results[0].keys():
            print("  âš ï¸ ê²½ê³ : ê¸°ì¡´ íŒŒì¼ì— EMP_NO ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë°ì´í„°ë¶€í„° EMP_NOê°€ í¬í•¨ë©ë‹ˆë‹¤.")
except FileNotFoundError:
    print("ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
except Exception as e:
    print(f"ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰): {e}")

results = existing_results.copy() if existing_results else []

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰ (ì°½ ìˆ¨ê¹€)
    context = browser.new_context()
    page = context.new_page()

    page.goto(EBSCO_URL, wait_until="domcontentloaded")
    # input("ê²€ìƒ‰ ê°€ëŠ¥í•œ ìƒíƒœë©´ Enter...")  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œì—ì„œëŠ” ë¶ˆí•„ìš”

    # ì¤‘ê°„ ì €ì¥ ì£¼ê¸° (5ê°œë§ˆë‹¤ ì €ì¥)
    save_interval = 5
    start_idx = len(results)  # ì´ë¯¸ ì²˜ë¦¬ëœ ê°œìˆ˜
    
    try:
        for i, q_dict in enumerate(queries, 1):
            q = q_dict["THSS_NM"]  # ê²€ìƒ‰ì–´ëŠ” THSS_NM
            emp_no = q_dict["EMP_NO"]  # EMP_NO ì €ì¥ìš©
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë…¼ë¬¸ì€ ê±´ë„ˆë›°ê¸°
            if str(q) in processed_titles:
                print(f"[{i}/{total_queries}] ì´ë¯¸ ì²˜ë¦¬ë¨ - ê±´ë„ˆëœ€: {q[:50]}...")
                continue
            
            print(f"[{i}/{total_queries}] Searching: {q}")

            try:
                # í˜„ì¬ URL í™•ì¸ - í™ˆí™”ë©´ì´ë©´ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                current_url = page.url
                if "search" not in current_url:
                    print(f"  â†’ í™ˆí™”ë©´ ê°ì§€, ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
                    page.goto(EBSCO_URL, wait_until="domcontentloaded")
                    time.sleep(1.5)
                
                # ê²€ìƒ‰ ì…ë ¥ í•„ë“œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                try:
                    search_input = page.locator("input#search-input")
                    if search_input.count() == 0:
                        print(f"  â†’ ê²€ìƒ‰ ì…ë ¥ í•„ë“œ ì—†ìŒ, ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
                        page.goto(EBSCO_URL, wait_until="domcontentloaded")
                        time.sleep(1.5)
                except:
                    print(f"  â†’ ê²€ìƒ‰ ì…ë ¥ í•„ë“œ í™•ì¸ ì‹¤íŒ¨, ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
                    page.goto(EBSCO_URL, wait_until="domcontentloaded")
                    time.sleep(1.5)

                # ê²€ìƒ‰ ì…ë ¥ í•„ë“œ í´ë¦¬ì–´ í›„ ê²€ìƒ‰
                try:
                    page.click("input#search-input")
                    time.sleep(0.5)
                    # ì „ì²´ ì„ íƒ í›„ ì‚­ì œ
                    page.keyboard.press("Control+A")
                    time.sleep(0.3)
                    page.keyboard.press("Backspace")
                    time.sleep(0.5)
                except:
                    pass
                
                # ê²€ìƒ‰ì–´ ì…ë ¥
                page.fill("input#search-input", q)
                time.sleep(0.5)
                page.press("input#search-input", "Enter")

                # ê²°ê³¼ ë¡œë”© ëŒ€ê¸° (ë” ì¶©ë¶„í•œ ëŒ€ê¸° ì‹œê°„)
                page.wait_for_load_state("networkidle")
                time.sleep(3.0)  # SPA ë Œë” ì•ˆì •í™” (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œì—ì„œëŠ” ë” ê¸´ ëŒ€ê¸° í•„ìš”)
                
                # ê²€ìƒ‰ ê²°ê³¼ ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ëŒ€ê¸° ì‹œë„ (ìµœëŒ€ 5ì´ˆ)
                try:
                    # ê²€ìƒ‰ ê²°ê³¼ ë˜ëŠ” "ê²°ê³¼ ì—†ìŒ" ë©”ì‹œì§€ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
                    page.wait_for_selector('h3[data-auto="result-item-title"], p:has-text("ì² ìë¥¼ í™•ì¸í•˜ê±°ë‚˜"), text=/ê²€ìƒ‰ ê²°ê³¼.*ê±´/', timeout=5000)
                except:
                    # ìš”ì†Œê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•„ë„ ê³„ì† ì§„í–‰ (ì´ë¯¸ networkidleë¡œ ì¶©ë¶„íˆ ëŒ€ê¸°í–ˆìœ¼ë¯€ë¡œ)
                    pass
                
                time.sleep(1.0)  # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸°

                # í™ˆí™”ë©´ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
                current_url_after = page.url
                if "search" not in current_url_after:
                    print(f"  â†’ ê²€ìƒ‰ í›„ í™ˆí™”ë©´ìœ¼ë¡œ ì´ë™ë¨ - ê²€ìƒ‰ ì‹¤íŒ¨")
                    results.append({
                        "EMP_NO": emp_no,
                        "THSS_NM": q,
                        "has_result": 0
                    })
                    processed_titles.add(str(q))
                    continue

                has_result = 0  # ê¸°ë³¸ê°’: ì—†ìŒ

                # âœ… ê²€ìƒ‰ ê²°ê³¼ ìˆìŒ í™•ì¸: ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„ (ì¬ì‹œë„ í¬í•¨)
                # ë°©ë²• 1: data-auto ì†ì„± ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
                result_cnt_1 = page.locator('h3[data-auto="result-item-title"]').count()
                result_cnt_link = page.locator('a[data-auto="result-item-title__link"]').count()
                
                # ë°©ë²• 2: mark íƒœê·¸ í™•ì¸ (ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ë•Œ í•˜ì´ë¼ì´íŠ¸ë˜ëŠ” mark íƒœê·¸)
                # h3 ë‚´ë¶€ì˜ mark íƒœê·¸ë§Œ í™•ì¸í•˜ì—¬ ë” ì •í™•í•˜ê²Œ íŒë‹¨
                result_cnt_mark_in_h3 = page.locator('h3[data-auto="result-item-title"] mark').count()
                result_cnt_mark_all = page.locator('mark').count()
                
                # ë°©ë²• 3: í´ë˜ìŠ¤ëª… íŒ¨í„´ ì‚¬ìš© (ë¶€ë¶„ ì¼ì¹˜ - ë°±ì—…ìš©)
                result_cnt_2 = page.locator('div[class*="result-item-header__title"]').count()
                result_cnt_3 = page.locator('div[class*="result-item-header"]').count()
                
                # ë°©ë²• 4: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ í…ìŠ¤íŠ¸ í™•ì¸ ("ê²€ìƒ‰ ê²°ê³¼: Xê±´")
                result_count_text = page.locator('text=/ê²€ìƒ‰ ê²°ê³¼.*ê±´/').count()
                
                # ì²« ë²ˆì§¸ ì‹œë„ì—ì„œ ì•„ë¬´ê²ƒë„ ì°¾ì§€ ëª»í•˜ë©´ ì¶”ê°€ ëŒ€ê¸° í›„ ì¬ì‹œë„
                if (result_cnt_1 == 0 and result_cnt_link == 0 and result_cnt_mark_in_h3 == 0 and 
                    result_cnt_mark_all < 2 and result_count_text == 0):
                    time.sleep(2.0)  # ì¶”ê°€ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    result_cnt_1 = page.locator('h3[data-auto="result-item-title"]').count()
                    result_cnt_link = page.locator('a[data-auto="result-item-title__link"]').count()
                    result_cnt_mark_in_h3 = page.locator('h3[data-auto="result-item-title"] mark').count()
                    result_cnt_mark_all = page.locator('mark').count()
                    result_cnt_2 = page.locator('div[class*="result-item-header__title"]').count()
                    result_cnt_3 = page.locator('div[class*="result-item-header"]').count()
                    result_count_text = page.locator('text=/ê²€ìƒ‰ ê²°ê³¼.*ê±´/').count()
                
                # ìµœì¢… ê²°ê³¼ ê°œìˆ˜ íŒë‹¨
                # h3 ìš”ì†Œê°€ ìˆê±°ë‚˜, ë§í¬ê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ê³  íŒë‹¨
                result_cnt = max(result_cnt_1, result_cnt_link, result_cnt_2, result_cnt_3)
                
                # h3 ë‚´ë¶€ì˜ mark íƒœê·¸ê°€ 1ê°œ ì´ìƒ ìˆìœ¼ë©´ í™•ì‹¤íˆ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ê³  íŒë‹¨
                # (ì œê³µëœ HTML êµ¬ì¡°: h3 ì•ˆì— ì—¬ëŸ¬ mark íƒœê·¸ê°€ ìˆìŒ)
                if result_cnt_mark_in_h3 >= 1:
                    result_cnt = max(result_cnt, 1)
                # ì „ì²´ mark íƒœê·¸ê°€ 2ê°œ ì´ìƒ ìˆìœ¼ë©´ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                elif result_cnt_mark_all >= 2:
                    result_cnt = max(result_cnt, 1)
                
                # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê²°ê³¼ê°€ ìˆë‹¤ê³  íŒë‹¨
                if result_count_text > 0:
                    result_cnt = max(result_cnt, 1)
                
                # âœ… ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ í™•ì¸: í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ í™•ì¸
                no_result_cnt = page.locator('text=/ì² ìë¥¼ í™•ì¸í•˜ê±°ë‚˜.*ê²€ìƒ‰í•˜ì‹­ì‹œì˜¤/').count()
                if no_result_cnt == 0:
                    no_result_cnt = page.locator('p:has-text("ì² ìë¥¼ í™•ì¸í•˜ê±°ë‚˜")').count()
                
                # í™ˆí™”ë©´ì¸ì§€ í™•ì¸ (ê²€ìƒ‰ ì…ë ¥ í•„ë“œê°€ ì—†ê±°ë‚˜ ê²€ìƒ‰ í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°)
                is_home_page = "search" not in page.url or page.locator("input#search-input").count() == 0

                # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                if result_cnt == 0 and no_result_cnt == 0:
                    print(f"  â†’ ë””ë²„ê¹…: h3-title={result_cnt_1}, link={result_cnt_link}, header-title={result_cnt_2}, header={result_cnt_3}, mark-in-h3={result_cnt_mark_in_h3}, mark-all={result_cnt_mark_all}, result-count-text={result_count_text}, no-result={no_result_cnt}, is-home={is_home_page}")

                if is_home_page:
                    has_result = 0
                    print("  â†’ í™ˆí™”ë©´ìœ¼ë¡œ ì´ë™í•¨ (ê²€ìƒ‰ ì‹¤íŒ¨) (0)")
                    results.append({
                        "EMP_NO": emp_no,
                        "THSS_NM": q,
                        "has_result": 0
                    })
                    processed_titles.add(str(q))  # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                elif no_result_cnt > 0:
                    has_result = 0
                    print("  â†’ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (0)")
                    # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒë„ JSONì— ì €ì¥ (ì¬ì‹¤í–‰ ì‹œ ë‹¤ì‹œ ê²€ìƒ‰í•˜ì§€ ì•Šë„ë¡)
                    results.append({
                        "EMP_NO": emp_no,
                        "THSS_NM": q,
                        "has_result": 0
                    })
                    processed_titles.add(str(q))  # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                elif result_cnt > 0 or result_cnt_mark_in_h3 >= 1 or result_cnt_mark_all >= 2 or result_count_text > 0:
                    has_result = 1
                    print(f"  â†’ ê²€ìƒ‰ ê²°ê³¼ ìˆìŒ (1) - h3={result_cnt_1}ê°œ, link={result_cnt_link}ê°œ, mark-in-h3={result_cnt_mark_in_h3}ê°œ, mark-all={result_cnt_mark_all}ê°œ, ê²°ê³¼í…ìŠ¤íŠ¸={result_count_text}ê°œ")
                    
                    # âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê²°ê³¼ì˜ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    paper_metadata = {"EMP_NO": emp_no, "THSS_NM": q, "has_result": 1}
                    
                    try:
                        # ì²« ë²ˆì§¸ ê²°ê³¼ ë§í¬ ì°¾ê¸°
                        first_link = page.locator('a[data-auto="result-item-title__link"]').first
                        if first_link.count() > 0:
                            # ìƒˆ íƒ­ì—ì„œ ì—´ê¸° (ë˜ëŠ” í˜„ì¬ í˜ì´ì§€ì—ì„œ ì´ë™)
                            href = first_link.get_attribute("href")
                            if href:
                                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                                if href.startswith("/"):
                                    detail_url = "https://research.ebsco.com" + href
                                else:
                                    detail_url = href
                                
                                # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
                                page.goto(detail_url, wait_until="domcontentloaded")
                                time.sleep(1.5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                                
                                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                                metadata_div = page.locator('div[data-auto="record-html-metadata"] article')
                                if metadata_div.count() > 0:
                                    # JavaScriptë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë” ì•ˆì •ì )
                                    metadata_dict = page.evaluate("""
                                        () => {
                                            const article = document.querySelector('div[data-auto="record-html-metadata"] article');
                                            if (!article) return {};
                                            
                                            const result = {};
                                            const h3Elements = article.querySelectorAll('h3');
                                            
                                            h3Elements.forEach(h3 => {
                                                const key = h3.textContent.trim();
                                                // h3 ë‹¤ìŒì˜ ì²« ë²ˆì§¸ ul ì°¾ê¸°
                                                let nextSibling = h3.nextElementSibling;
                                                while (nextSibling && nextSibling.tagName !== 'UL') {
                                                    nextSibling = nextSibling.nextElementSibling;
                                                }
                                                
                                                if (nextSibling && nextSibling.tagName === 'UL') {
                                                    const liElements = nextSibling.querySelectorAll('li');
                                                    const values = Array.from(liElements).map(li => li.textContent.trim()).filter(v => v);
                                                    if (values.length > 0) {
                                                        result[key] = values.length === 1 ? values[0] : values;
                                                    }
                                                }
                                            });
                                            
                                            return result;
                                        }
                                    """)
                                    
                                    # ì¶”ì¶œí•œ ë©”íƒ€ë°ì´í„°ë¥¼ paper_metadataì— ì¶”ê°€
                                    paper_metadata.update(metadata_dict)
                                    
                                    extracted_fields = [k for k in paper_metadata.keys() if k not in ['THSS_NM', 'has_result', 'metadata_error', 'EMP_NO']]
                                    print(f"  â†’ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(extracted_fields)}ê°œ í•„ë“œ ({', '.join(extracted_fields[:3])}{'...' if len(extracted_fields) > 3 else ''})")
                                else:
                                    print("  â†’ ë©”íƒ€ë°ì´í„° ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                                    paper_metadata["metadata_error"] = "ë©”íƒ€ë°ì´í„° ìš”ì†Œ ì—†ìŒ"
                                
                                # ê²€ìƒ‰ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸°
                                page.goto(EBSCO_URL, wait_until="domcontentloaded")
                                time.sleep(0.5)
                            else:
                                print("  â†’ ë§í¬ hrefë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                                paper_metadata["metadata_error"] = "ë§í¬ ì—†ìŒ"
                        else:
                            print("  â†’ ê²°ê³¼ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                            paper_metadata["metadata_error"] = "ë§í¬ ìš”ì†Œ ì—†ìŒ"
                            
                    except Exception as e:
                        print(f"  â†’ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        paper_metadata["metadata_error"] = f"ì˜¤ë¥˜: {str(e)}"
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸° ì‹œë„
                        try:
                            if "search" not in page.url:
                                page.goto(EBSCO_URL, wait_until="domcontentloaded")
                                time.sleep(0.5)
                        except:
                            pass
                    
                    results.append(paper_metadata)
                    processed_titles.add(str(q))  # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                    
                else:
                    has_result = 0
                    print("  â†’ ê²°ê³¼ íŒë³„ ì‹¤íŒ¨(ë¡œë”©/êµ¬ì¡° ë¬¸ì œ) (0) - JSONì— ì €ì¥í•˜ì§€ ì•ŠìŒ")
                    # ê²°ê³¼ íŒë³„ ì‹¤íŒ¨ì¸ ê²½ìš° resultsì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ê¸°ë¡ë§Œ í•˜ê³  ì €ì¥ì€ ì•ˆ í•¨)
                    processed_titles.add(str(q))  # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œë§Œ (ì¤‘ë³µ ë°©ì§€)

                # ì£¼ê¸°ì ìœ¼ë¡œ ì¤‘ê°„ ì €ì¥ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë°ì´í„° ì†ì‹¤ ë°©ì§€)
                # ê²°ê³¼ íŒë³„ ì‹¤íŒ¨ëŠ” ì €ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ resultsì—ëŠ” í•­ìƒ ìœ íš¨í•œ ë°ì´í„°ë§Œ ìˆìŒ
                current_processed = len(results) - start_idx
                if current_processed > 0 and current_processed % save_interval == 0:
                    try:
                        with open(output_file, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=2)
                        print(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({len(results)}ê°œ ê²°ê³¼ ì €ì¥ë¨)")
                    except Exception as e:
                        print(f"  âš ï¸ ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")

                # ë‹¤ìŒ ê²€ìƒ‰ ì¤€ë¹„ (í™ˆí™”ë©´ì´ ì•„ë‹ ë•Œë§Œ í´ë¦¬ì–´)
                current_url_before_clear = page.url
                if "search" in current_url_before_clear and page.locator("input#search-input").count() > 0:
                    try:
                        clear_btn = page.locator('button[aria-label="Clear"]')
                        if clear_btn.count() > 0:
                            clear_btn.click()
                            time.sleep(0.5)
                        else:
                            # Clear ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ í´ë¦¬ì–´
                            page.click("input#search-input")
                            time.sleep(0.3)
                            page.keyboard.press("Control+A")
                            time.sleep(0.2)
                            page.keyboard.press("Backspace")
                            time.sleep(0.3)
                    except Exception as e:
                        # í´ë¦¬ì–´ ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ìœ¼ë¡œ í´ë¦¬ì–´
                        try:
                            page.click("input#search-input")
                            time.sleep(0.3)
                            page.keyboard.press("Control+A")
                            time.sleep(0.2)
                            page.keyboard.press("Backspace")
                            time.sleep(0.3)
                        except:
                            pass

                time.sleep(2.0)  # ê²€ìƒ‰ ê°„ê²© (ë„ˆë¬´ ë¹ ë¥´ë©´ ì„œë²„ê°€ ì°¨ë‹¨í•  ìˆ˜ ìˆìŒ, 2ì´ˆë©´ ì¶©ë¶„)
                
            except Exception as e:
                # ê°œë³„ ë…¼ë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œ ê³„ì† ì§„í–‰)
                print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ (ë‹¤ìŒìœ¼ë¡œ ì§„í–‰): {str(e)}")
                error_result = {
                    "EMP_NO": emp_no,
                    "THSS_NM": q,
                    "has_result": 0,
                    "metadata_error": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                }
                results.append(error_result)
                processed_titles.add(str(q))  # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥
                current_processed = len(results) - start_idx
                if current_processed > 0 and current_processed % save_interval == 0:
                    try:
                        with open(output_file, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=2)
                        print(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({len(results)}ê°œ ê²°ê³¼ ì €ì¥ë¨)")
                    except Exception as save_error:
                        print(f"  âš ï¸ ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                
                continue  # ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œ ê³„ì†

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise  # KeyboardInterruptëŠ” ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ ì •ìƒ ì¢…ë£Œ ì²˜ë¦¬
        
    except Exception as e:
        print(f"\n\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
        
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        try:
            browser.close()
        except:
            pass
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥ (ì˜¤ë¥˜ ë°œìƒí•´ë„ ì €ì¥)
        # ê²°ê³¼ íŒë³„ ì‹¤íŒ¨ëŠ” resultsì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì €ì¥í•  ë°ì´í„°ë§Œ ìˆìŒ
        try:
            if results:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"\nâœ… ìµœì¢… ê²°ê³¼ë¥¼ '{output_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤. (ì´ {len(results)}ê°œ)")
                print("\n=== ê²€ìƒ‰ ê²°ê³¼ ë° ë©”íƒ€ë°ì´í„° (ì¼ë¶€) ===")
                # JSON ë°ì´í„°ì˜ ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                for i, item in enumerate(results[:10], 1):
                    print(f"{i}. {item.get('THSS_NM', 'N/A')[:50]}... (has_result: {item.get('has_result', 0)})")
                if len(results) > 10:
                    print(f"... ì™¸ {len(results) - 10}ê°œ")
            else:
                print("\nâš ï¸ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"\nâŒ ìµœì¢… ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ìˆ˜ë™ìœ¼ë¡œ results ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        finally:
            # DB ì—°ê²° ì¢…ë£Œ
            close_db_connection(conn)

