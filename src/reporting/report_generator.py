"""
Report Generator
AHP ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„±
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from openai import OpenAI

sys.path.append(str(Path(__file__).parent.parent.parent))
from config.settings import OPENAI_API_KEY, LLM_MODEL
from src.utils.cost_tracker import log_chat_usage, get_cost_tracker


class ReportGenerator:
    """ì‚°í•™ ë§¤ì¹­ ì¶”ì²œ ë³´ê³ ì„œ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = None, api_key: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            output_dir: ë³´ê³ ì„œ ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ results/test/report ì‚¬ìš©)
            api_key: OpenAI API í‚¤ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        """
        if output_dir is None:
            self.output_dir = Path("results/test/report")
        else:
            self.output_dir = Path(output_dir)
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = api_key or OPENAI_API_KEY
        if not api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. config/settings.pyì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        
        self.client = OpenAI(api_key=api_key)
        self.model = LLM_MODEL
    
    def generate_report_from_query(
        self,
        query: str,
        doc_types: List[str] = None,
        few_shot_examples: Optional[List[Dict[str, Any]]] = None,
        retrieval_top_k: int = None,
        retriever = None
    ) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (RAG â†’ AHP â†’ ë¦¬í¬íŠ¸ ìƒì„±)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            doc_types: ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ["patent", "article", "project"])
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
            retrieval_top_k: Local/Global ê²€ìƒ‰ ì‹œ ê°ê° ê°€ì ¸ì˜¬ ê°œìˆ˜ (ê¸°ë³¸: 5)
            retriever: ì™¸ë¶€ì—ì„œ ì£¼ì…í•  HybridRetriever ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ë‚´ë¶€ ìƒì„±)

        Returns:
            ìƒì„±ëœ ë¦¬í¬íŠ¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        if doc_types is None:
            doc_types = ["patent", "article", "project"]

        # ë¹„ìš© ì¶”ì  ì‹œì‘
        tracker = get_cost_tracker()
        tracker.start_task("full_pipeline", description=f"ì „ì²´ íŒŒì´í”„ë¼ì¸: {query[:30]}...")

        # RAG ê²€ìƒ‰ â†’ êµìˆ˜ ì§‘ê³„ â†’ AHP ë­í‚¹ â†’ ë¦¬í¬íŠ¸ ìƒì„±
        from src.rag.query.retriever import HybridRetriever
        from src.ranking.professor_aggregator import ProfessorAggregator
        from src.ranking.ranker import ProfessorRanker
        from config.settings import RETRIEVAL_TOP_K, SIMILARITY_THRESHOLD
        from config.ahp_config import DEFAULT_TYPE_WEIGHTS

        # 1. RAG ê²€ìƒ‰ (ì™¸ë¶€ ì£¼ì… ë˜ëŠ” ë‚´ë¶€ ìƒì„±)
        print("RAG ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
        if retriever is None:
            retriever = HybridRetriever(doc_types=doc_types)
        raw_rag_results = retriever.retrieve(
            query=query,
            retrieval_top_k=retrieval_top_k or RETRIEVAL_TOP_K,
            similarity_threshold=SIMILARITY_THRESHOLD,
            mode="hybrid"
        )

        # RAG ê²°ê³¼ë¥¼ test_rag.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì—”í‹°í‹°/ê´€ê³„ ì •ë³´ ë³´ì¡´)
        rag_results = self._convert_rag_results(raw_rag_results)

        # 2. êµìˆ˜ë³„ ì§‘ê³„
        print("êµìˆ˜ë³„ ë¬¸ì„œ ì§‘ê³„ ì¤‘...")
        aggregator = ProfessorAggregator()
        professor_data = aggregator.aggregate_by_professor(
            rag_results=rag_results,
            doc_types=doc_types
        )
        
        # 3. AHP ë­í‚¹
        print("AHP ê¸°ë°˜ êµìˆ˜ ìˆœìœ„ í‰ê°€ ì¤‘...")
        ranker = ProfessorRanker()
        ranked_professors = ranker.rank_professors(professor_data, DEFAULT_TYPE_WEIGHTS)
        
        # 4. AHP ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        ahp_results = {
            "query": query,
            "keywords": rag_results.get("keywords", {}),
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "total_professors": len(ranked_professors),
            "type_weights": DEFAULT_TYPE_WEIGHTS,
            "ranked_professors": ranked_professors
        }
        
        # 5. ë¦¬í¬íŠ¸ ìƒì„±
        result = self.generate_report(
            ahp_results=ahp_results,
            rag_results=rag_results,
            few_shot_examples=few_shot_examples
        )

        # ë¹„ìš© ì¶”ì  ì¢…ë£Œ
        cost_result = tracker.end_task()
        if cost_result:
            result["api_cost"] = cost_result

        return result
    
    def generate_report(
        self,
        ahp_results: Dict[str, Any],
        rag_results: Optional[Dict[str, Any]] = None,
        few_shot_examples: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        AHP ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            ahp_results: AHP ê²°ê³¼ JSON (ahp_results_*.json íŒŒì¼ ë‚´ìš©)
            rag_results: RAG ê²€ìƒ‰ ê²°ê³¼ (ì—”í‹°í‹°/ê´€ê³„ ì •ë³´ ì¶”ì¶œìš©, Noneì´ë©´ ahp_resultsì—ì„œ ì¶”ì¶œ ì‹œë„)
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
                - í˜•ì‹: [{"input": {...}, "output": "..."}, ...]
                - ë˜ëŠ”: {"examples": [{"input": {...}, "output": "..."}]}
                - ì˜ˆì‹œ íŒŒì¼: data/report_few_shot_examples.json
                - Noneì´ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
            
        Returns:
            ìƒì„±ëœ ë¦¬í¬íŠ¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        # ì…ë ¥ JSON ì¤€ë¹„
        input_json = self._prepare_input_json(ahp_results, rag_results)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt(input_json, few_shot_examples)
        
        # GPT-4o-mini í˜¸ì¶œ
        print("GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ êµìˆ˜ ì¶”ì²œ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë³´ê³ ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )

        # ë¹„ìš© ì¶”ì 
        log_chat_usage(
            component="report_generation",
            model=self.model,
            response=response
        )

        report_text = response.choices[0].message.content
        
        # ê²°ê³¼ êµ¬ì¡°í™”
        report_data = {
            "query": ahp_results.get("query", ""),
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "report_text": report_text,
            "input_data": input_json,
            "model": self.model
        }
        
        return report_data
    
    def _convert_rag_results(self, raw_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        HybridRetriever ê²°ê³¼ë¥¼ test_rag.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        (query.pyì˜ save_query_result ë¡œì§ê³¼ ë™ì¼)

        Args:
            raw_results: HybridRetriever.retrieve() ê²°ê³¼

        Returns:
            test_rag.json í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        docs_dict = {}

        # local_results ì²˜ë¦¬
        for r in raw_results.get('local_results', []):
            no = str(r.get('metadata', {}).get('source_doc_id', ''))
            if not no:
                continue

            doc_type = r.get('doc_type', 'unknown')

            if no not in docs_dict:
                docs_dict[no] = {
                    "no": no,
                    "data_type": doc_type,
                    "matches": []
                }

            match_info = {
                "search_type": "local",
                "similarity": r.get('similarity', 0),
                "matched_entity": {
                    "name": r.get('metadata', {}).get('name', ''),
                    "entity_type": r.get('metadata', {}).get('entity_type', ''),
                    "description": r.get('document', '')
                },
                "neighbors_1hop": [
                    {
                        "name": n.get('name', ''),
                        "entity_type": n.get('entity_type', ''),
                        "relation_keywords": n.get('relation_keywords', []),
                        "relation_description": n.get('relation_description', '')
                    }
                    for n in r.get('neighbors', [])
                ]
            }
            docs_dict[no]["matches"].append(match_info)

        # global_results ì²˜ë¦¬
        for r in raw_results.get('global_results', []):
            no = str(r.get('metadata', {}).get('source_doc_id', ''))
            if not no:
                continue

            doc_type = r.get('doc_type', 'unknown')

            if no not in docs_dict:
                docs_dict[no] = {
                    "no": no,
                    "data_type": doc_type,
                    "matches": []
                }

            match_info = {
                "search_type": "global",
                "similarity": r.get('similarity', 0),
                "matched_relation": {
                    "source_entity": r.get('metadata', {}).get('source_entity', ''),
                    "target_entity": r.get('metadata', {}).get('target_entity', ''),
                    "keywords": r.get('metadata', {}).get('keywords', ''),
                    "description": r.get('document', '')
                },
                "source_entity_info": r.get('source_entity_info'),
                "target_entity_info": r.get('target_entity_info')
            }
            docs_dict[no]["matches"].append(match_info)

        # matches ë‚´ë¶€ similarity ê¸°ì¤€ ì •ë ¬
        for doc in docs_dict.values():
            doc['matches'] = sorted(
                doc['matches'],
                key=lambda m: m.get('similarity', 0),
                reverse=True
            )

        # dict â†’ list ë³€í™˜ í›„ ì •ë ¬
        retrieved_docs = sorted(
            docs_dict.values(),
            key=lambda doc: max((m.get('similarity', 0) for m in doc['matches']), default=0),
            reverse=True
        )

        return {
            "query": raw_results.get('query', ''),
            "keywords": {
                "high_level": raw_results.get('high_level_keywords', []),
                "low_level": raw_results.get('low_level_keywords', [])
            },
            "retrieved_docs": retrieved_docs
        }

    def _prepare_input_json(
        self,
        ahp_results: Dict[str, Any],
        rag_results: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ì…ë ¥ JSON ì¤€ë¹„
        
        Args:
            ahp_results: AHP ê²°ê³¼
            rag_results: RAG ê²°ê³¼ (Noneì´ë©´ ì—”í‹°í‹°/ê´€ê³„ ì •ë³´ ì—†ì´ ìƒì„±)
            
        Returns:
            ë¦¬í¬íŠ¸ ìƒì„±ìš© ì…ë ¥ JSON
        """
        query = ahp_results.get("query", "")
        keywords = ahp_results.get("keywords", {})
        
        # í‚¤ì›Œë“œ ì •ë³´ ì¤€ë¹„ (high_level, low_level í™œìš©)
        high_level_keywords = keywords.get("high_level", [])
        low_level_keywords = keywords.get("low_level", [])
        
        # ì—”í‹°í‹°ì™€ ê´€ê³„ ì¶”ì¶œ (ë¬¸ì„œë³„ ì—”í‹°í‹°/ê´€ê³„ëŠ” ìœ ì§€, ì „ì²´ ì¶”ì¶œì€ í‚¤ì›Œë“œë¡œ ëŒ€ì²´)
        extracted_relationships = []
        
        if rag_results:
            # RAG ê²°ê³¼ì—ì„œ ê´€ê³„ë§Œ ì¶”ì¶œ (ì—”í‹°í‹°ëŠ” í‚¤ì›Œë“œë¡œ ëŒ€ì²´)
            retrieved_docs = rag_results.get("retrieved_docs", [])
            relation_set = set()
            
            for doc in retrieved_docs:
                matches = doc.get("matches", [])
                for match in matches:
                    # ê´€ê³„ ì¶”ì¶œ
                    relation = match.get("matched_relation", {})
                    if relation:
                        relation_key = f"{relation.get('source_entity', '')} -> {relation.get('target_entity', '')}"
                        if relation_key not in relation_set:
                            extracted_relationships.append({
                                "source": relation.get("source_entity", ""),
                                "target": relation.get("target_entity", ""),
                                "description": relation.get("description", ""),
                                "keywords": relation.get("keywords", "")
                            })
                            relation_set.add(relation_key)
        
        # êµìˆ˜ë³„ ë¬¸ì„œ ì •ë³´ ì¤€ë¹„ (ìµœëŒ€ 3ëª…)
        professors_data = []
        ranked_professors = ahp_results.get("ranked_professors", [])[:3]  # ìƒìœ„ 3ëª…ë§Œ
        
        for prof in ranked_professors:
            prof_info = prof.get("professor_info", {})
            documents = prof.get("documents", {})
            document_scores = prof.get("document_scores", {})
            scores_by_type = prof.get("scores_by_type", {})
            total_score = prof.get("total_score", 0.0)
            
            prof_docs = []
            
            # ê° ë¬¸ì„œ íƒ€ì…ë³„ë¡œ ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘ (ìµœëŒ€ 3ê°œì”©)
            for doc_type in ["patent", "article", "project"]:
                docs = documents.get(doc_type, [])
                doc_scores_list = document_scores.get(doc_type, [])
                
                # ë¬¸ì„œ ì ìˆ˜ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë¹ ë¥¸ ì¡°íšŒìš©)
                score_dict = {}
                for doc_score in doc_scores_list:
                    doc_no = str(doc_score.get("no", ""))
                    score_dict[doc_no] = doc_score.get("score", 0.0)
                
                # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìˆœ)
                docs_with_scores = []
                for doc in docs:
                    doc_no = str(doc.get("no", ""))
                    score = score_dict.get(doc_no, 0.0)
                    docs_with_scores.append((doc, score))
                
                # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ 3ê°œë§Œ ì„ íƒ
                docs_with_scores.sort(key=lambda x: x[1], reverse=True)
                selected_docs = docs_with_scores[:3]
                
                for doc, score in selected_docs:
                    # ë¬¸ì„œ ìš”ì•½ (text í•„ë“œì˜ ì¼ë¶€)
                    text = doc.get("text", "")
                    summary = text[:200] + "..." if len(text) > 200 else text
                    
                    # ì—”í‹°í‹°ì™€ ê´€ê³„ ì¶”ì¶œ (í•´ë‹¹ ë¬¸ì„œì˜ matchesì—ì„œ)
                    doc_entities = []
                    doc_relations = []
                    
                    # ë¬¸ì„œ ë²ˆí˜¸ë¡œ RAG ê²°ê³¼ì—ì„œ ë§¤ì¹­ ì •ë³´ ì°¾ê¸°
                    doc_no = str(doc.get("no", ""))
                    if rag_results:
                        retrieved_docs = rag_results.get("retrieved_docs", [])
                        for rag_doc in retrieved_docs:
                            # ë¬¸ì„œ ë²ˆí˜¸ ë¹„êµ (ë¬¸ìì—´ë¡œ í†µì¼)
                            rag_doc_no = str(rag_doc.get("no", ""))
                            if rag_doc_no == doc_no:
                                matches = rag_doc.get("matches", [])
                                for match in matches:
                                    # source_entity_infoì™€ target_entity_infoì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
                                    source_info = match.get("source_entity_info", {})
                                    target_info = match.get("target_entity_info", {})
                                    
                                    if source_info and source_info.get("name"):
                                        doc_entities.append(source_info["name"])
                                    if target_info and target_info.get("name"):
                                        doc_entities.append(target_info["name"])
                                    
                                    # matched_relationì—ì„œ ê´€ê³„ ì¶”ì¶œ
                                    relation = match.get("matched_relation", {})
                                    if relation:
                                        source_entity = relation.get("source_entity", "")
                                        target_entity = relation.get("target_entity", "")
                                        if source_entity and target_entity:
                                            doc_relations.append(f"{source_entity} -> {target_entity}")
                                break
                    
                    prof_docs.append({
                        "type": doc_type,
                        "title": doc.get("title", ""),
                        "summary": summary,
                        "year": doc.get("year", ""),
                        "score": score,  # AHP ì ìˆ˜ ì¶”ê°€
                        "entities": list(set(doc_entities)),
                        "relationships": list(set(doc_relations))
                    })
            
            professors_data.append({
                "name": prof_info.get("NM", ""),
                "department": f"{prof_info.get('COLG_NM', '')} {prof_info.get('HG_NM', '')}".strip(),
                "total_score": total_score,  # êµìˆ˜ ì¢…í•© ì ìˆ˜
                "scores_by_type": scores_by_type,  # íƒ€ì…ë³„ ì ìˆ˜
                "documents": prof_docs
            })
        
        input_json = {
            "query": query,
            "keywords": {
                "high_level": high_level_keywords,
                "low_level": low_level_keywords
            },
            "extracted_relationships": extracted_relationships,
            "professors": professors_data
        }
        
        return input_json
    
    def _build_prompt(
        self,
        input_json: Dict[str, Any],
        few_shot_examples: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        
        Args:
            input_json: ì…ë ¥ JSON ë°ì´í„°
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
                - ê° ì˜ˆì‹œëŠ” {"input": {...}, "output": "..."} í˜•ì‹
                - ë³´ê³ ì„œ ìƒì„± í˜•ì‹ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì˜ˆì‹œ ë°ì´í„°
                - ì°¸ê³  íŒŒì¼: data/report_few_shot_examples.json
            
        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        base_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ êµìˆ˜ ì¶”ì²œ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë³´ê³ ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì§ˆì˜ì™€ ì¶”ì²œëœ êµìˆ˜ ë° ê´€ë ¨ ë¬¸ì„œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
ì•„ë˜ì— ëª…ì‹œëœ ë³´ê³ ì„œ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ êµ¬ì¡°í™”ëœ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë°˜ë“œì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
- ì…ë ¥ JSON ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ì™¸ë¶€ ì§€ì‹ì´ë‚˜ í•´ì„ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
- ìƒˆë¡œìš´ ì •ë³´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ì¶”ë¡ í•˜ì§€ ë§ˆì„¸ìš”.
- ì£¼ê´€ì ì¸ í‰ê°€, ì¶”ì²œ ë¬¸ì¥, í•´ì„ í‘œí˜„ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- í‘œì™€ ë¬¸ì¥ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš” (Streamlitì—ì„œ í‘œì‹œë©ë‹ˆë‹¤).
- êµìˆ˜ëŠ” ìµœëŒ€ 3ëª…ê¹Œì§€ë§Œ í‘œì‹œí•˜ì„¸ìš” (ì´ë¯¸ ì…ë ¥ JSONì— ìƒìœ„ 3ëª…ë§Œ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤).
- ê° êµìˆ˜ì˜ ë¬¸ì„œëŠ” ë°ì´í„° ìœ í˜•(patent/article/project)ë³„ë¡œ ìµœëŒ€ 3ê°œì”©ë§Œ í‘œì‹œí•˜ì„¸ìš” (ì´ë¯¸ ì…ë ¥ JSONì— ìƒìœ„ 3ê°œì”©ë§Œ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤).

---

### [ğŸ“„ ë³´ê³ ì„œ ì¶œë ¥ í˜•ì‹ ì§€ì¹¨]

## 1. ì‚¬ìš©ì ê²€ìƒ‰ ì •ë³´

**ì…ë ¥ ì§ˆì˜:** ì…ë ¥ JSONì˜ "query" í•„ë“œ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.

**ê³ ìˆ˜ì¤€ í‚¤ì›Œë“œ:** ì…ë ¥ JSONì˜ "keywords.high_level" ë°°ì—´ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”. ì˜ˆ: "ë”¥ëŸ¬ë‹ ì˜ë£Œì˜ìƒ ë¶„ì„, ì˜ë£Œì˜ìƒ ì²˜ë¦¬ ì‹œìŠ¤í…œ ê°œë°œ"

**ì €ìˆ˜ì¤€ í‚¤ì›Œë“œ:** ì…ë ¥ JSONì˜ "keywords.low_level" ë°°ì—´ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”. ì˜ˆ: "ë”¥ëŸ¬ë‹, ì˜ë£Œì˜ìƒ, ì˜ìƒ ë¶„ì„, ì‹œìŠ¤í…œ"

**ì¶”ì¶œ ê´€ê³„:** ì…ë ¥ JSONì˜ "extracted_relationships" ë°°ì—´ì—ì„œ ê° ê´€ê³„ì˜ "source"ì™€ "target"ì„ "A -> B" í˜•ì‹ìœ¼ë¡œ ë‚˜ì—´í•˜ì„¸ìš”. 
- ì˜ˆ: "ìì—°ìŠ¤ëŸ¬ìš´ í™”ì§ˆ ë³µì› -> ë² ì´ì–´ ë””ëª¨ìì´í¬ ë°©ë²•, ë² ì´ì–´ ë””ëª¨ìì´í¬ ë°©ë²• -> ë² ì´ì–´ CFA íŒ¨í„´"
- ë°°ì—´ì´ ë¹„ì–´ìˆê±°ë‚˜ ì—†ìœ¼ë©´ "ì—†ìŒ" ë˜ëŠ” ìƒëµí•˜ì„¸ìš”.

## 2. êµìˆ˜ë³„ ê´€ë ¨ ë¬¸ì„œ ëª©ë¡

(ì•„ë˜ í˜•ì‹ì„ êµìˆ˜ 1ëª…ë‹¹ ë°˜ë³µí•´ì„œ ì‘ì„±í•˜ì„¸ìš”. ìµœëŒ€ 3ëª…ê¹Œì§€ë§Œ)

ì…ë ¥ JSONì˜ "professors" ë°°ì—´ì—ì„œ ê° êµìˆ˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

### êµìˆ˜ëª…: [professors[].name]
**ì†Œì†:** [professors[].department]  
**ì¢…í•© ì ìˆ˜:** [professors[].total_score] (AHP ì¢…í•© ì ìˆ˜)  
**íƒ€ì…ë³„ ì ìˆ˜:** íŠ¹í—ˆ=[professors[].scores_by_type.patent], ë…¼ë¬¸=[professors[].scores_by_type.article], ì—°êµ¬ê³¼ì œ=[professors[].scores_by_type.project]

| ë¬¸ì„œ ìœ í˜• | ì œëª© | ì—°ë„ | ìš”ì•½ | AHPì ìˆ˜ | ê°œì²´ | ê´€ê³„ |
|-----------|------|------|------|---------|------|------|
| [documents[].type] | [documents[].title] | [documents[].year] | [documents[].summary] | [documents[].score] | [documents[].entities] | [documents[].relationships] |

**ì£¼ì˜ì‚¬í•­:**
- ê° êµìˆ˜ë§ˆë‹¤ patent, article, project ê°ê° ìµœëŒ€ 3ê°œì”©ë§Œ í‘œì‹œí•˜ì„¸ìš”.
- ë¬¸ì„œëŠ” AHP ì ìˆ˜ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- ê°œì²´(entities)ëŠ” í•´ë‹¹ ë¬¸ì„œì˜ "entities" ë°°ì—´ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”. ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ì¹¸.
- ê´€ê³„(relationships)ëŠ” í•´ë‹¹ ë¬¸ì„œì˜ "relationships" ë°°ì—´ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”. ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ì¹¸.

## 3. ë³´ê³ ì„œ ì„¤ëª…

ëª¨ë“  ì •ë³´ëŠ” ì…ë ¥ëœ ë¬¸ì„œ ìš”ì•½ ë° êµ¬ì¡°í™”ëœ ì •ë³´ì—ì„œ ì¶”ì¶œë˜ì—ˆìœ¼ë©°, ì£¼ê´€ì ì¸ í•´ì„ì´ë‚˜ íŒë‹¨ì€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
êµìˆ˜ëŠ” AHP ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ 3ëª…ë§Œ í¬í•¨ë˜ì—ˆìœ¼ë©°, ê° êµìˆ˜ì˜ ë¬¸ì„œëŠ” ë°ì´í„° ìœ í˜•ë³„ë¡œ ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 3ê°œì”©ë§Œ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.

---
"""
        
        # Few-shot ì˜ˆì‹œ ì¶”ê°€
        if few_shot_examples:
            for i, example in enumerate(few_shot_examples, 1):
                example_input = example.get("input", {})
                example_output = example.get("output", "")
                
                base_prompt += f"""
### [âœ… Few-shot ì˜ˆì‹œ {i}]

ì…ë ¥ JSON:
{json.dumps(example_input, ensure_ascii=False, indent=2)}

ì¶œë ¥ ë³´ê³ ì„œ:
{example_output}

---
"""
        
        # ìµœì¢… ì…ë ¥ JSON ì¶”ê°€
        base_prompt += f"""
### [ğŸ§¾ ìƒˆë¡­ê²Œ ì‘ì„±í•´ì•¼ í•  ë³´ê³ ì„œ ëŒ€ìƒ JSON]

ë‹¤ìŒ JSON ë°ì´í„°ë¥¼ ìœ„ì™€ ë™ì¼í•œ í˜•ì‹(ì˜ˆì‹œ 1~2 ì°¸ì¡°)ì— ë”°ë¼ êµ¬ì¡°í™”ëœ ë³´ê³ ì„œë¡œ ì‘ì„±í•˜ì„¸ìš”:

{json.dumps(input_json, ensure_ascii=False, indent=2)}
"""
        
        return base_prompt
    
    def save_json(
        self,
        report_data: Dict[str, Any],
        filename: str = None
    ) -> Path:
        """
        JSON í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì €ì¥
        
        Args:
            report_data: ë³´ê³ ì„œ ë°ì´í„°
            filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filename is None:
            timestamp = report_data.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            filename = f"report_{timestamp}.json"
        
        file_path = self.output_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        return file_path
    
    def save_text(
        self,
        report_data: Dict[str, Any],
        filename: str = None
    ) -> Path:
        """
        í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì €ì¥
        
        Args:
            report_data: ë³´ê³ ì„œ ë°ì´í„°
            filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filename is None:
            timestamp = report_data.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            filename = f"report_{timestamp}.txt"
        
        file_path = self.output_dir / filename
        
        report_text = report_data.get("report_text", "")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        return file_path
