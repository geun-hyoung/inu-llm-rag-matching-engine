"""
Report Generator
AHP ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„±
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from openai import OpenAI

sys.path.append(str(Path(__file__).parent.parent.parent))
from config.settings import OPENAI_API_KEY, LLM_MODEL
from src.utils.cost_tracker import log_chat_usage, get_cost_tracker


class ReportGenerator:
    """ì‚°í•™ ë§¤ì¹­ ì¶”ì²œ ë³´ê³ ì„œ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = None, api_key: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            output_dir: ë³´ê³ ì„œ ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ results/runs/report ì‚¬ìš©)
            api_key: OpenAI API í‚¤ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        """
        if output_dir is None:
            self.output_dir = Path("results/runs/report")
        else:
            self.output_dir = Path(output_dir)
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = api_key or OPENAI_API_KEY
        if not api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. config/settings.pyì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        
        self.client = OpenAI(api_key=api_key)
        self.model = LLM_MODEL
    
    def generate_report_from_query(
        self,
        query: str,
        doc_types: List[str] = None,
        few_shot_examples: Optional[List[Dict[str, Any]]] = None,
        retrieval_top_k: int = None,
        retriever = None
    ) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (RAG â†’ AHP â†’ ë¦¬í¬íŠ¸ ìƒì„±)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            doc_types: ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ["patent", "article", "project"])
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
            retrieval_top_k: Local/Global ê²€ìƒ‰ ì‹œ ê°ê° ê°€ì ¸ì˜¬ ê°œìˆ˜ (ê¸°ë³¸: 5)
            retriever: ì™¸ë¶€ì—ì„œ ì£¼ì…í•  HybridRetriever ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ë‚´ë¶€ ìƒì„±)

        Returns:
            ìƒì„±ëœ ë¦¬í¬íŠ¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        if doc_types is None:
            doc_types = ["patent", "article", "project"]

        # ë¹„ìš© ì¶”ì  ì‹œì‘
        tracker = get_cost_tracker()
        tracker.start_task("full_pipeline", description=f"ì „ì²´ íŒŒì´í”„ë¼ì¸: {query[:30]}...")

        # RAG ê²€ìƒ‰ â†’ êµìˆ˜ ì§‘ê³„ â†’ AHP ë­í‚¹ â†’ ë¦¬í¬íŠ¸ ìƒì„±
        from src.rag.query.retriever import HybridRetriever
        from src.ranking.professor_aggregator import ProfessorAggregator
        from src.ranking.ranker import ProfessorRanker
        from config.settings import RETRIEVAL_TOP_K, SIMILARITY_THRESHOLD
        from config.ahp_config import DEFAULT_TYPE_WEIGHTS

        # 1. RAG ê²€ìƒ‰ (ì™¸ë¶€ ì£¼ì… ë˜ëŠ” ë‚´ë¶€ ìƒì„±)
        print("RAG ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
        if retriever is None:
            retriever = HybridRetriever(doc_types=doc_types)
        raw_rag_results = retriever.retrieve(
            query=query,
            retrieval_top_k=retrieval_top_k or RETRIEVAL_TOP_K,
            similarity_threshold=SIMILARITY_THRESHOLD,
            mode="hybrid"
        )

        # RAG ê²°ê³¼ë¥¼ test_rag.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì—”í‹°í‹°/ê´€ê³„ ì •ë³´ ë³´ì¡´)
        rag_results = self._convert_rag_results(raw_rag_results)

        # 2. êµìˆ˜ë³„ ì§‘ê³„
        print("êµìˆ˜ë³„ ë¬¸ì„œ ì§‘ê³„ ì¤‘...")
        aggregator = ProfessorAggregator()
        professor_data = aggregator.aggregate_by_professor(
            rag_results=rag_results,
            doc_types=doc_types
        )
        
        # 3. AHP ë­í‚¹
        print("AHP ê¸°ë°˜ êµìˆ˜ ìˆœìœ„ í‰ê°€ ì¤‘...")
        ranker = ProfessorRanker()
        ranked_professors = ranker.rank_professors(professor_data, DEFAULT_TYPE_WEIGHTS)
        
        # 4. AHP ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•œ ë²ˆì˜ ì‹¤í–‰ì—ì„œ RAG/AHP/REPORT ë¡œê·¸ìš© ë™ì¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©)
        run_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        ahp_results = {
            "query": query,
            "keywords": rag_results.get("keywords", {}),
            "timestamp": run_ts,
            "total_professors": len(ranked_professors),
            "type_weights": DEFAULT_TYPE_WEIGHTS,
            "ranked_professors": ranked_professors
        }

        # 5. ë¦¬í¬íŠ¸ ìƒì„±
        result = self.generate_report(
            ahp_results=ahp_results,
            rag_results=rag_results,
            few_shot_examples=few_shot_examples
        )
        result["timestamp"] = run_ts
        result["rag_results"] = rag_results
        result["ahp_results"] = ahp_results

        # ë¹„ìš© ì¶”ì  ì¢…ë£Œ
        cost_result = tracker.end_task()
        if cost_result:
            result["api_cost"] = cost_result

        return result
    
    def generate_report(
        self,
        ahp_results: Dict[str, Any],
        rag_results: Optional[Dict[str, Any]] = None,
        few_shot_examples: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        AHP ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            ahp_results: AHP ê²°ê³¼ JSON (ahp_results_*.json íŒŒì¼ ë‚´ìš©)
            rag_results: RAG ê²€ìƒ‰ ê²°ê³¼ (ì—”í‹°í‹°/ê´€ê³„ ì •ë³´ ì¶”ì¶œìš©, Noneì´ë©´ ahp_resultsì—ì„œ ì¶”ì¶œ ì‹œë„)
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
                - í˜•ì‹: [{"input": {...}, "output": "..."}, ...]
                - ë˜ëŠ”: {"examples": [{"input": {...}, "output": "..."}]}
                - ì˜ˆì‹œ íŒŒì¼: data/report_few_shot_examples.json
                - Noneì´ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
            
        Returns:
            ìƒì„±ëœ ë¦¬í¬íŠ¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        # ì…ë ¥ JSON ì¤€ë¹„
        input_json = self._prepare_input_json(ahp_results, rag_results)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt(input_json, few_shot_examples)
        
        # GPT-4o-mini í˜¸ì¶œ
        print("GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ êµìˆ˜ ì¶”ì²œ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë³´ê³ ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )

        # ë¹„ìš© ì¶”ì 
        log_chat_usage(
            component="report_generation",
            model=self.model,
            response=response
        )

        report_text = response.choices[0].message.content
        
        # ê²°ê³¼ êµ¬ì¡°í™”
        report_data = {
            "query": ahp_results.get("query", ""),
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "report_text": report_text,
            "input_data": input_json,
            "model": self.model
        }
        
        return report_data
    
    def _convert_rag_results(self, raw_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        HybridRetriever ê²°ê³¼ë¥¼ test_rag.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        (query.pyì˜ save_query_result ë¡œì§ê³¼ ë™ì¼)

        Args:
            raw_results: HybridRetriever.retrieve() ê²°ê³¼

        Returns:
            test_rag.json í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        docs_dict = {}

        # local_results ì²˜ë¦¬
        for r in raw_results.get('local_results', []):
            no = str(r.get('metadata', {}).get('source_doc_id', ''))
            if not no:
                continue

            doc_type = r.get('doc_type', 'unknown')

            if no not in docs_dict:
                docs_dict[no] = {
                    "no": no,
                    "data_type": doc_type,
                    "matches": []
                }

            match_info = {
                "search_type": "local",
                "similarity": r.get('similarity', 0),
                "matched_entity": {
                    "name": r.get('metadata', {}).get('name', ''),
                    "entity_type": r.get('metadata', {}).get('entity_type', ''),
                    "description": r.get('document', '')
                },
                "neighbors_1hop": [
                    {
                        "name": n.get('name', ''),
                        "entity_type": n.get('entity_type', ''),
                        "relation_keywords": n.get('relation_keywords', []),
                        "relation_description": n.get('relation_description', '')
                    }
                    for n in r.get('neighbors', [])
                ]
            }
            docs_dict[no]["matches"].append(match_info)

        # global_results ì²˜ë¦¬
        for r in raw_results.get('global_results', []):
            no = str(r.get('metadata', {}).get('source_doc_id', ''))
            if not no:
                continue

            doc_type = r.get('doc_type', 'unknown')

            if no not in docs_dict:
                docs_dict[no] = {
                    "no": no,
                    "data_type": doc_type,
                    "matches": []
                }

            match_info = {
                "search_type": "global",
                "similarity": r.get('similarity', 0),
                "matched_relation": {
                    "source_entity": r.get('metadata', {}).get('source_entity', ''),
                    "target_entity": r.get('metadata', {}).get('target_entity', ''),
                    "keywords": r.get('metadata', {}).get('keywords', ''),
                    "description": r.get('document', '')
                },
                "source_entity_info": r.get('source_entity_info'),
                "target_entity_info": r.get('target_entity_info')
            }
            docs_dict[no]["matches"].append(match_info)

        # matches ë‚´ë¶€ similarity ê¸°ì¤€ ì •ë ¬
        for doc in docs_dict.values():
            doc['matches'] = sorted(
                doc['matches'],
                key=lambda m: m.get('similarity', 0),
                reverse=True
            )

        # dict â†’ list ë³€í™˜ í›„ ì •ë ¬
        retrieved_docs = sorted(
            docs_dict.values(),
            key=lambda doc: max((m.get('similarity', 0) for m in doc['matches']), default=0),
            reverse=True
        )

        return {
            "query": raw_results.get('query', ''),
            "keywords": {
                "high_level": raw_results.get('high_level_keywords', []),
                "low_level": raw_results.get('low_level_keywords', [])
            },
            "retrieved_docs": retrieved_docs
        }

    def _prepare_input_json(
        self,
        ahp_results: Dict[str, Any],
        rag_results: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ì…ë ¥ JSON ì¤€ë¹„.

        ë³´ê³ ì„œì— í‘œì‹œë˜ëŠ” í•­ëª© ì¶œì²˜:
        - query, keywords.high_level/low_level: RAG ê²€ìƒ‰ ê²°ê³¼(ì¿¼ë¦¬Â·í‚¤ì›Œë“œ ì¶”ì¶œ)
        - extracted_relationships: RAG retrieved_docsì˜ matchesì—ì„œ ì¶”ì¶œ
          (global: matched_relation, local: matched_entity + neighbors_1hop). ìƒìœ„ Nê°œë§Œ ì‚¬ìš©.
        - professors: AHP ranked_professors ìƒìœ„ 3ëª…
        - ê° êµìˆ˜ documents: AHP documents(patent/article/project) ìœ í˜•ë³„ ìƒìœ„ 3ê°œ,
          entities/relationships: í•´ë‹¹ ë¬¸ì„œ noì™€ ì¼ì¹˜í•˜ëŠ” RAG matchesì—ì„œë§Œ ì¶”ì¶œ. ë¬¸ì„œë‹¹ ìƒìœ„ Nê°œë§Œ.

        Args:
            ahp_results: AHP ê²°ê³¼
            rag_results: RAG ê²°ê³¼ (Noneì´ë©´ ì—”í‹°í‹°/ê´€ê³„ ì •ë³´ ì—†ì´ ìƒì„±)

        Returns:
            ë¦¬í¬íŠ¸ ìƒì„±ìš© ì…ë ¥ JSON
        """
        # ë³´ê³ ì„œ í•­ëª© ê°œìˆ˜ ì œí•œ (ë¬´ë¶„ë³„í•˜ê²Œ ë§ì•„ì§€ì§€ ì•Šë„ë¡)
        MAX_EXTRACTED_RELATIONSHIPS = 25
        MAX_ENTITIES_PER_DOC = 10
        MAX_RELATIONSHIPS_PER_DOC = 10

        query = ahp_results.get("query", "")
        keywords = ahp_results.get("keywords", {})

        high_level_keywords = keywords.get("high_level", [])
        low_level_keywords = keywords.get("low_level", [])

        # RAG ê²°ê³¼ì—ì„œ ì¶”ì¶œëœ ê´€ê³„ (ìƒìœ„ Nê°œë§Œ)
        extracted_relationships = []
        if rag_results:
            retrieved_docs = rag_results.get("retrieved_docs", [])
            relation_set = set()
            for doc in retrieved_docs:
                for match in doc.get("matches", []):
                    if len(extracted_relationships) >= MAX_EXTRACTED_RELATIONSHIPS:
                        break
                    rel = match.get("matched_relation", {})
                    if rel:
                        relation_key = f"{rel.get('source_entity', '')} -> {rel.get('target_entity', '')}"
                        if relation_key not in relation_set:
                            extracted_relationships.append({
                                "source": rel.get("source_entity", ""),
                                "target": rel.get("target_entity", ""),
                                "description": rel.get("description", ""),
                                "keywords": rel.get("keywords", "")
                            })
                            relation_set.add(relation_key)
                    ent = match.get("matched_entity", {}) or {}
                    if ent.get("name"):
                        for n in match.get("neighbors_1hop", []):
                            if len(extracted_relationships) >= MAX_EXTRACTED_RELATIONSHIPS:
                                break
                            nname = n.get("name", "")
                            if nname:
                                relation_key = f"{ent['name']} -> {nname}"
                                if relation_key not in relation_set:
                                    extracted_relationships.append({
                                        "source": ent["name"],
                                        "target": nname,
                                        "description": n.get("relation_description", ""),
                                        "keywords": ", ".join(n.get("relation_keywords", []))
                                    })
                                    relation_set.add(relation_key)
                if len(extracted_relationships) >= MAX_EXTRACTED_RELATIONSHIPS:
                    break

        # êµìˆ˜ë³„ ë¬¸ì„œ ì •ë³´ ì¤€ë¹„ (ìµœëŒ€ 3ëª…)
        professors_data = []
        ranked_professors = ahp_results.get("ranked_professors", [])[:3]
        
        for idx, prof in enumerate(ranked_professors, 1):
            prof_info = prof.get("professor_info", {})
            documents = prof.get("documents", {})
            document_scores = prof.get("document_scores", {})
            
            prof_docs = []
            
            for doc_type in ["patent", "article", "project"]:
                docs = documents.get(doc_type, [])
                doc_scores_list = document_scores.get(doc_type, [])
                score_dict = {str(ds.get("no", "")): ds.get("score", 0.0) for ds in doc_scores_list}
                docs_with_scores = [(doc, score_dict.get(str(doc.get("no", "")), 0.0)) for doc in docs]
                docs_with_scores.sort(key=lambda x: x[1], reverse=True)
                selected_docs = docs_with_scores[:3]
                
                for doc, _ in selected_docs:
                    text = doc.get("text", "")
                    summary = text[:200] + "..." if len(text) > 200 else text
                    doc_no = str(doc.get("no", ""))
                    
                    # ì—”í‹°í‹°/ê´€ê³„: RAG retrieved_docsì˜ matchesì—ì„œ ì¶”ì¶œ (global + local ëª¨ë‘)
                    doc_entities = []
                    doc_relations = []
                    if rag_results:
                        for rag_doc in rag_results.get("retrieved_docs", []):
                            if str(rag_doc.get("no", "")) != doc_no:
                                continue
                            for match in rag_doc.get("matches", []):
                                rel = match.get("matched_relation", {})
                                if rel:
                                    s, t = rel.get("source_entity", ""), rel.get("target_entity", "")
                                    if s:
                                        doc_entities.append(s)
                                    if t:
                                        doc_entities.append(t)
                                    if s and t:
                                        doc_relations.append(f"{s} -> {t}")
                                for info in (match.get("source_entity_info"), match.get("target_entity_info")):
                                    if info and info.get("name"):
                                        doc_entities.append(info["name"])
                                ent = match.get("matched_entity", {}) or {}
                                if ent.get("name"):
                                    doc_entities.append(ent["name"])
                                    for n in match.get("neighbors_1hop", []):
                                        nname = n.get("name", "")
                                        if nname:
                                            doc_entities.append(nname)
                                            doc_relations.append(f"{ent['name']} -> {nname}")
                            break

                    prof_docs.append({
                        "type": doc_type,
                        "title": doc.get("title", ""),
                        "summary": summary,
                        "year": doc.get("year", ""),
                        "entities": list(set(doc_entities))[:MAX_ENTITIES_PER_DOC],
                        "relationships": list(set(doc_relations))[:MAX_RELATIONSHIPS_PER_DOC]
                    })
            
            professors_data.append({
                "number": idx,
                "name": prof_info.get("NM", ""),
                "department": f"{prof_info.get('COLG_NM', '')} {prof_info.get('HG_NM', '')}".strip(),
                "contact": prof_info.get("EMAIL", ""),
                "documents": prof_docs
            })
        
        input_json = {
            "query": query,
            "keywords": {
                "high_level": high_level_keywords,
                "low_level": low_level_keywords
            },
            "extracted_relationships": extracted_relationships,
            "professors": professors_data
        }
        
        return input_json
    
    def _build_prompt(
        self,
        input_json: Dict[str, Any],
        few_shot_examples: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        
        Args:
            input_json: ì…ë ¥ JSON ë°ì´í„°
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
                - ê° ì˜ˆì‹œëŠ” {"input": {...}, "output": "..."} í˜•ì‹
                - ë³´ê³ ì„œ ìƒì„± í˜•ì‹ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì˜ˆì‹œ ë°ì´í„°
                - ì°¸ê³  íŒŒì¼: data/report_few_shot_examples.json
            
        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        base_prompt = """ë‹¹ì‹ ì€ ì‚°í•™í˜‘ë ¥ ë§¤ì¹­ì„ ìœ„í•œ **ê³µì‹ ì¶”ì²œ ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì…ë ¥ëœ ê²€ìƒ‰ ì§ˆì˜ì™€ ì¶”ì²œ êµìˆ˜Â·ë¬¸ì„œ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬, ì•„ë˜ í˜•ì‹ì— ë§ì¶° **ì •ëˆëœ ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

[ì§€ì¹¨]
- ì…ë ¥ JSONì˜ ê°’ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ë¡ Â·í•´ì„Â·í‰ê°€ ë¬¸ì¥ì„ ë„£ì§€ ë§ˆì„¸ìš”.
- êµìˆ˜ëŠ” ë°˜ë“œì‹œ "1. OOO êµìˆ˜", "2. OOO êµìˆ˜", "3. OOO êµìˆ˜" í˜•ì‹ìœ¼ë¡œ ë²ˆí˜¸ì™€ í•¨ê»˜ í‘œê¸°í•˜ì„¸ìš”.
- AHP ì ìˆ˜Â·ì¢…í•© ì ìˆ˜ëŠ” ë³´ê³ ì„œì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í‘œëŠ” ë°˜ë“œì‹œ íŒŒì´í”„(|) í…Œì´ë¸” í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ê° êµìˆ˜ë‹¹ ë¬¸ì„œëŠ” patent/article/project ìœ í˜•ë³„ ìµœëŒ€ 3ê°œì”©ë§Œ í‘œì‹œí•˜ì„¸ìš”.

---

### [ë³´ê³ ì„œ ì¶œë ¥ í˜•ì‹]

ë³´ê³ ì„œ **ë§¨ ìœ„**ì— ë‹¤ìŒ ì œëª© ë¸”ë¡ì„ ë„£ìœ¼ì„¸ìš” (ë‚ ì§œëŠ” ë³´ê³ ì„œ ìƒì„±ì¼ë¡œ ë¹„ìŠ·í•˜ê²Œ):

---
# ì‚°í•™ ë§¤ì¹­ ì¶”ì²œ ë³´ê³ ì„œ

**ì‘ì„±ì¼:** (í˜„ì¬ ì—°ë„-ì›”-ì¼ í˜•ì‹)  
**ê²€ìƒ‰ ì§ˆì˜:** (ì…ë ¥ JSONì˜ query ê°’)
---

ê·¸ ë‹¤ìŒ ì•„ë˜ ì„¹ì…˜ì„ **ìˆœì„œëŒ€ë¡œ** ì‘ì„±í•˜ì„¸ìš”.

---

## 1. ê²€ìƒ‰ ê°œìš”

- **ê³ ìˆ˜ì¤€ í‚¤ì›Œë“œ:** (keywords.high_level ë°°ì—´ì„ ì‰¼í‘œë¡œ ë‚˜ì—´)
- **ì €ìˆ˜ì¤€ í‚¤ì›Œë“œ:** (keywords.low_level ë°°ì—´ì„ ì‰¼í‘œë¡œ ë‚˜ì—´)
- **ì¶”ì¶œëœ ê°œì²´Â·ê´€ê³„:** extracted_relationshipsì—ì„œ "source -> target" í˜•ì‹ìœ¼ë¡œ ë‚˜ì—´. ì—†ìœ¼ë©´ "í•´ë‹¹ ì—†ìŒ"

---

## 2. ì¶”ì²œ êµìˆ˜ ë° ê´€ë ¨ ë¬¸ì„œ

professors ë°°ì—´ì„ ìˆœì„œëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ê° êµìˆ˜ ë¸”ë¡ì€ ì•„ë˜ì™€ ê°™ì´ ì‘ì„±í•˜ì„¸ìš”.

### 1. [ì´ë¦„] êµìˆ˜
- **ì†Œì†:** (department)
- **ì—°ë½ ìˆ˜ë‹¨:** (contact ì´ë©”ì¼, ì—†ìœ¼ë©´ "-")

| ë¬¸ì„œ ìœ í˜• | ì œëª© | ì—°ë„ | ìš”ì•½ | ê°œì²´ | ê´€ê³„ |
|:----------|------|:----:|------|------|------|
| (type) | (title) | (year) | (summary ì¼ë¶€) | (entities ì‰¼í‘œ êµ¬ë¶„) | (relationships ì‰¼í‘œ êµ¬ë¶„) |

(2ë²ˆ, 3ë²ˆ êµìˆ˜ë„ ë™ì¼ í˜•ì‹ìœ¼ë¡œ ë°˜ë³µ)

---

## 3. ì•ˆë‚´

ë³¸ ë³´ê³ ì„œì˜ ë‚´ìš©ì€ ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½ ë° ì¶”ì¶œëœ ê°œì²´Â·ê´€ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## 4. ì‚°í•™í˜‘ë ¥ë‹¨ ë¬¸ì˜

ì‚°í•™í˜‘ë ¥ ê´€ë ¨ ë¬¸ì˜ëŠ” ì•„ë˜ ì—°ë½ì²˜ë¡œ ì´ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

| êµ¬ë¶„ | ë‚´ìš© |
|------|------|
| ë‹´ë‹¹ì | ê¹€OO |
| ì´ë©”ì¼ | oo@inu.ac.kr |
| ì—°ë½ì²˜ | 032-835-0000 |

---
"""
        
        # Few-shot ì˜ˆì‹œ ì¶”ê°€
        if few_shot_examples:
            for i, example in enumerate(few_shot_examples, 1):
                example_input = example.get("input", {})
                example_output = example.get("output", "")
                
                base_prompt += f"""
### [âœ… Few-shot ì˜ˆì‹œ {i}]

ì…ë ¥ JSON:
{json.dumps(example_input, ensure_ascii=False, indent=2)}

ì¶œë ¥ ë³´ê³ ì„œ:
{example_output}

---
"""
        
        # ìµœì¢… ì…ë ¥ JSON ì¶”ê°€
        base_prompt += f"""
### [ğŸ§¾ ìƒˆë¡­ê²Œ ì‘ì„±í•´ì•¼ í•  ë³´ê³ ì„œ ëŒ€ìƒ JSON]

ë‹¤ìŒ JSON ë°ì´í„°ë¥¼ ìœ„ì™€ ë™ì¼í•œ í˜•ì‹(ì˜ˆì‹œ 1~2 ì°¸ì¡°)ì— ë”°ë¼ êµ¬ì¡°í™”ëœ ë³´ê³ ì„œë¡œ ì‘ì„±í•˜ì„¸ìš”:

{json.dumps(input_json, ensure_ascii=False, indent=2)}
"""
        
        return base_prompt
    
    def save_json(
        self,
        report_data: Dict[str, Any],
        filename: str = None
    ) -> Path:
        """
        JSON í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì €ì¥
        
        Args:
            report_data: ë³´ê³ ì„œ ë°ì´í„°
            filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filename is None:
            timestamp = report_data.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            filename = f"report_{timestamp}.json"
        
        file_path = self.output_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        return file_path
    
    def save_text(
        self,
        report_data: Dict[str, Any],
        filename: str = None
    ) -> Path:
        """
        í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì €ì¥
        
        Args:
            report_data: ë³´ê³ ì„œ ë°ì´í„°
            filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filename is None:
            timestamp = report_data.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            filename = f"report_{timestamp}.txt"
        
        file_path = self.output_dir / filename
        
        report_text = report_data.get("report_text", "")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        return file_path

    def save_pdf(
        self,
        report_data: Dict[str, Any],
        filename: str = None
    ) -> Optional[Path]:
        """
        PDF í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì €ì¥ (fpdf2 ì‚¬ìš©, GTK ë¶ˆí•„ìš”).
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ. ì‹¤íŒ¨ ì‹œ None.
        """
        if filename is None:
            timestamp = report_data.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            filename = f"report_{timestamp}.pdf"
        file_path = self.output_dir / filename
        report_text = report_data.get("report_text", "")
        return self._save_pdf_fpdf2(file_path, report_text)

    @staticmethod
    def _emoji_to_text(s: str) -> str:
        """PDFì—ì„œ ì´ëª¨í‹°ì½˜ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ í”í•œ ì´ëª¨í‹°ì½˜ì„ [í…ìŠ¤íŠ¸]ë¡œ ì¹˜í™˜."""
        if not s:
            return s
        replace_map = {
            "âœ…": "[ì²´í¬]", "âœ“": "[ì²´í¬]", "âœ”": "[ì²´í¬]",
            "âŒ": "[ì˜¤ë¥˜]", "âœ—": "[X]",
            "âš ï¸": "[ì£¼ì˜]", "âš ": "[ì£¼ì˜]",
            "ğŸ“‹": "[ë³´ê³ ì„œ]", "ğŸ“„": "[ë¬¸ì„œ]", "ğŸ“": "[í´ë”]",
            "ğŸ”": "[ê²€ìƒ‰]", "ğŸ“¥": "[ë‹¤ìš´ë¡œë“œ]", "ğŸš€": "[ì‹¤í–‰]",
            "ğŸ“Œ": "[í•€]", "ğŸ’¡": "[ì•„ì´ë””ì–´]", "ğŸ“Š": "[ì°¨íŠ¸]",
            "ğŸ‘‰": "[ì°¸ê³ ]", "â€¢": "Â·", "â€“": "-", "â€”": "-",
        }
        out = s
        for emoji, text in replace_map.items():
            out = out.replace(emoji, text)
        # ë‚˜ë¨¸ì§€ ì´ëª¨í‹°ì½˜ ë²”ìœ„(ëŒ€ëµ)ëŠ” ê³µë°± ë˜ëŠ” [?]ë¡œ (ì„ íƒ)
        return out

    @staticmethod
    def _parse_md_blocks(lines: List[str]) -> List[Dict[str, Any]]:
        """ë§ˆí¬ë‹¤ìš´ ì¤„ì„ ì¼ë°˜ ì¤„ / í‘œ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ”. í‘œëŠ” |...| í˜•íƒœ ì—°ì† ì¤„."""
        blocks: List[Dict[str, Any]] = []
        i = 0
        while i < len(lines):
            line = lines[i]
            if not line.strip():
                blocks.append({"type": "line", "content": ""})
                i += 1
                continue
            # í‘œ í–‰: ë§¨ ì•ì´ | ì´ê³  ì¤‘ê°„ì— | ê°€ ìˆìŒ
            if line.strip().startswith("|") and line.count("|") >= 2:
                table_rows: List[List[str]] = []
                while i < len(lines) and lines[i].strip().startswith("|") and lines[i].count("|") >= 2:
                    row_line = lines[i]
                    parts = [p.strip() for p in row_line.split("|")]
                    if len(parts) > 2:
                        cells = parts[1:-1]
                    else:
                        cells = [p for p in parts if p]
                    if not cells:
                        i += 1
                        continue
                    is_sep = all(all(ch in " \t:-" for ch in cell) for cell in cells)
                    if not is_sep:
                        table_rows.append(cells)
                    i += 1
                if table_rows:
                    blocks.append({"type": "table", "rows": table_rows})
                continue
            blocks.append({"type": "line", "content": line})
            i += 1
        return blocks

    def _save_pdf_fpdf2(self, file_path: Path, report_text: str) -> Optional[Path]:
        """fpdf2ë¡œ PDF ìƒì„±. ë§ˆí¬ë‹¤ìš´(# í—¤ë”©, **êµµê²Œ), í‘œ(í…Œì´ë¸”), ì´ëª¨í‹°ì½˜â†’í…ìŠ¤íŠ¸ ë°˜ì˜."""
        try:
            from fpdf import FPDF  # type: ignore[import-untyped]
        except ImportError:
            print("PDF ì €ì¥ì„ ìœ„í•´ pip install fpdf2 ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None

        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        pdf.add_page()
        pdf.set_margins(20, 15, 20)
        usable_w = pdf.w - pdf.l_margin - pdf.r_margin
        if usable_w <= 0:
            pdf.set_margins(10, 10, 10)
            usable_w = pdf.w - pdf.l_margin - pdf.r_margin
        pdf.set_xy(pdf.l_margin, pdf.t_margin)

        # í•œê¸€ í°íŠ¸: ë§‘ì€ ê³ ë”• (ì¼ë°˜ + ë³¼ë“œ). ë§ˆí¬ë‹¤ìš´ ** êµµê²Œìš©.
        font_added = False
        font_has_bold = False
        for (reg, bold) in [
            ("C:/Windows/Fonts/malgun.ttf", "C:/Windows/Fonts/malgunbd.ttf"),
            (Path.home() / "AppData/Local/Microsoft/Windows/Fonts/malgun.ttf", Path.home() / "AppData/Local/Microsoft/Windows/Fonts/malgunbd.ttf"),
        ]:
            pr = Path(reg) if isinstance(reg, str) else reg
            pb = Path(bold) if isinstance(bold, str) else bold
            if pr.exists():
                try:
                    pdf.add_font("Malgun", "", str(pr))
                    if pb.exists():
                        pdf.add_font("Malgun", "B", str(pb))
                        font_has_bold = True
                    pdf.set_font("Malgun", "", 10)
                    font_added = True
                    break
                except Exception:
                    continue

        # ì´ëª¨í‹°ì½˜ í´ë°± í°íŠ¸ (ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ _emoji_to_textë¡œ ì¹˜í™˜)
        for emoji_path in [
            "C:/Windows/Fonts/seguiemj.ttf",
            Path.home() / "AppData/Local/Microsoft/Windows/Fonts/seguiemj.ttf",
        ]:
            pe = Path(emoji_path) if isinstance(emoji_path, str) else emoji_path
            if pe.exists():
                try:
                    pdf.add_font("SegoeEmoji", "", str(pe))
                    pdf.set_fallback_fonts(["segoeemoji"], exact_match=False)
                    break
                except Exception:
                    pass

        if not font_added:
            pdf.set_font("Helvetica", "", 10)

        cell_w = max(usable_w * 0.95, 50.0)
        raw_lines = [ln.rstrip() for ln in report_text.replace("\r", "").split("\n")]
        blocks = self._parse_md_blocks(raw_lines)

        def get_heading_style(line: str):
            """ì¤„ ì• # ê°œìˆ˜ì— ë”°ë¼ (í°íŠ¸ í¬ê¸°, ì¤„ë†’ì´, ì œê±°í•  ë¬¸ì ìˆ˜, ë³¼ë“œ ì—¬ë¶€)."""
            if line.startswith("### "):
                return (11, 7.5, 4, True)
            if line.startswith("## "):
                return (13, 8.0, 3, True)
            if line.startswith("# "):
                return (16, 9.0, 2, True)
            return (10, 6.5, 0, False)

        def render_blocks(pdf_obj, use_helvetica_only: bool):
            base_size = 10
            if use_helvetica_only:
                pdf_obj.set_font("Helvetica", "", base_size)
            elif font_added:
                pdf_obj.set_font("Malgun", "", base_size)
            markdown_ok = use_helvetica_only or (font_added and font_has_bold)

            for blk in blocks:
                if blk["type"] == "line":
                    line = blk["content"]
                    if not line:
                        pdf_obj.ln(6)
                        continue
                    pdf_obj.set_x(pdf_obj.l_margin)

                    size, line_h, strip_len, is_heading = get_heading_style(line)
                    content = line[strip_len:].lstrip() if strip_len else line
                    content = self._emoji_to_text(content)
                    txt = (content.encode("latin-1", errors="replace").decode("latin-1") + "\n") if use_helvetica_only else (content + "\n")

                    if is_heading:
                        pdf_obj.ln(2)
                    if size != base_size or (is_heading and font_has_bold and not use_helvetica_only):
                        if use_helvetica_only:
                            pdf_obj.set_font("Helvetica", "B" if is_heading else "", size)
                        elif font_added:
                            pdf_obj.set_font("Malgun", "B" if (is_heading and font_has_bold) else "", size)

                    try:
                        pdf_obj.multi_cell(w=cell_w, h=line_h, txt=txt, new_x="LMARGIN", new_y="NEXT", markdown=markdown_ok)
                    except Exception:
                        pdf_obj.set_x(pdf_obj.l_margin)
                        pdf_obj.set_font("Helvetica", "B" if is_heading else "", size)
                        safe_txt = content.encode("latin-1", errors="replace").decode("latin-1") + "\n"
                        pdf_obj.multi_cell(w=cell_w, h=line_h, txt=safe_txt, new_x="LMARGIN", new_y="NEXT", markdown=False)
                        if font_added and not use_helvetica_only:
                            pdf_obj.set_font("Malgun", "B" if (is_heading and font_has_bold) else "", size)

                    if is_heading:
                        pdf_obj.ln(2)
                    if size != base_size or (is_heading and font_has_bold and not use_helvetica_only):
                        if use_helvetica_only:
                            pdf_obj.set_font("Helvetica", "", base_size)
                        elif font_added:
                            pdf_obj.set_font("Malgun", "", base_size)

                elif blk["type"] == "table":
                    rows = blk["rows"]
                    if not rows:
                        continue
                    ncols = max(len(r) for r in rows)
                    padded = [list(r) + [""] * (ncols - len(r)) for r in rows]
                    pdf_obj.set_x(pdf_obj.l_margin)
                    pdf_obj.ln(3)
                    try:
                        with pdf_obj.table(
                            width=cell_w,
                            first_row_as_headings=True,
                            markdown=False,
                            line_height=6.5,
                            padding=3,
                            num_heading_rows=1,
                        ) as table:
                            for row in padded:
                                cells = [self._emoji_to_text(str(c)) for c in row]
                                if use_helvetica_only:
                                    cells = [c.encode("latin-1", errors="replace").decode("latin-1") for c in cells]
                                table.row(cells=cells)
                    except Exception:
                        # í…Œì´ë¸” ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ fallback
                        for row in padded:
                            fallback_line = " | ".join(self._emoji_to_text(str(c)) for c in row)
                            pdf_obj.set_x(pdf_obj.l_margin)
                            pdf_obj.multi_cell(w=cell_w, h=6, txt=fallback_line + "\n", new_x="LMARGIN", new_y="NEXT", markdown=False)
                    pdf_obj.ln(3)

        try:
            render_blocks(pdf, use_helvetica_only=False)
        except Exception as e:
            if "horizontal space" in str(e).lower():
                pdf = FPDF()
                pdf.set_auto_page_break(auto=True, margin=15)
                pdf.add_page()
                pdf.set_margins(20, 15, 20)
                pdf.set_xy(pdf.l_margin, pdf.t_margin)
                pdf.set_font("Helvetica", "", 10)
                render_blocks(pdf, use_helvetica_only=True)
            else:
                raise

        try:
            pdf.output(str(file_path))
            return file_path
        except Exception as e:
            print(f"fpdf2 PDF ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
