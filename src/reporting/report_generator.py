"""
Report Generator
AHP ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„±
"""

import asyncio
import json
import re
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from openai import OpenAI

sys.path.append(str(Path(__file__).parent.parent.parent))
from config.settings import OPENAI_API_KEY, LLM_MODEL
try:
    from config.settings import (
        REPORT_FEW_SHOT_MAX_EXAMPLES,
        REPORT_SUMMARY_MAX_CHARS,
        REPORT_MAX_TOKENS,
    )
except ImportError:
    REPORT_FEW_SHOT_MAX_EXAMPLES = None
    REPORT_SUMMARY_MAX_CHARS = 500
    REPORT_MAX_TOKENS = 4096
from src.utils.cost_tracker import log_chat_usage, get_cost_tracker


def _escape_html(s: str) -> str:
    """HTML ì´ìŠ¤ì¼€ì´í”„ (fallbackìš©)."""
    if not s:
        return ""
    import html as _html
    return _html.escape(s)


def normalize_keywords_if_duplicate_query(keywords: Dict[str, Any], query: str) -> Dict[str, List[str]]:
    """
    retrieverê°€ ì‹¤íŒ¨í•´ high_level/low_level ë‘˜ ë‹¤ [query]ë¡œ ì˜¨ ê²½ìš°ë¥¼ ì •ê·œí™”.
    ì €ìˆ˜ì¤€ì€ ì§ˆì˜ì—ì„œ í† í°ì„ ì¶”ì¶œí•˜ê³ , ê³ ìˆ˜ì¤€ì€ ì§ˆì˜ 1ê°œë§Œ ìœ ì§€í•´ ì¤‘ë³µ í‘œì‹œë¥¼ ë§‰ìŒ.
    """
    high = list(keywords.get("high_level") or [])
    low = list(keywords.get("low_level") or [])
    if not query or (len(high) != 1 or len(low) != 1):
        return {"high_level": high, "low_level": low}
    if high[0] != query or low[0] != query:
        return {"high_level": high, "low_level": low}

    # ë‘˜ ë‹¤ [query] â†’ ì €ìˆ˜ì¤€ë§Œ ì§ˆì˜ì—ì„œ í† í° ë¶„ë¦¬ (2ê¸€ì ì´ìƒ, ì¢…ê²°ì–´ ì œì™¸)
    stop = {"ì°¾ê³ ", "ìˆì–´", "í•´ìš”", "í•´ì£¼ì‹¤", "ìˆë‚˜ìš”", "ìˆì–´ìš”", "ì‹¶ì–´", "ë¶€íƒ", "ë“œë ¤ìš”"}
    tokens = [t.strip() for t in re.split(r"[\s,]+", query) if len(t.strip()) >= 2]
    tokens = [t for t in tokens if t not in stop][:6]
    if not tokens:
        tokens = [query]
    return {"high_level": [query], "low_level": tokens}


class ReportGenerator:
    """ì‚°í•™ ë§¤ì¹­ ì¶”ì²œ ë³´ê³ ì„œ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = None, api_key: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            output_dir: ë³´ê³ ì„œ ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ results/runs/report ì‚¬ìš©)
            api_key: OpenAI API í‚¤ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        """
        if output_dir is None:
            self.output_dir = Path("results/runs/report")
        else:
            self.output_dir = Path(output_dir)
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = api_key or OPENAI_API_KEY
        if not api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. config/settings.pyì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        
        self.client = OpenAI(api_key=api_key)
        self.model = LLM_MODEL
    
    def generate_report_from_query(
        self,
        query: str,
        doc_types: List[str] = None,
        few_shot_examples: Optional[List[Dict[str, Any]]] = None,
        retrieval_top_k: int = None,
        retriever = None
    ) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (RAG â†’ AHP â†’ ë¦¬í¬íŠ¸ ìƒì„±)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            doc_types: ê²€ìƒ‰í•  ë¬¸ì„œ íƒ€ì… ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ["patent", "article", "project"])
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
            retrieval_top_k: Local/Global ê²€ìƒ‰ ì‹œ ê°ê° ê°€ì ¸ì˜¬ ê°œìˆ˜ (ê¸°ë³¸: 5)
            retriever: ì™¸ë¶€ì—ì„œ ì£¼ì…í•  HybridRetriever ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ë‚´ë¶€ ìƒì„±)

        Returns:
            ìƒì„±ëœ ë¦¬í¬íŠ¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        if doc_types is None:
            doc_types = ["patent", "article", "project"]

        # ë¹„ìš© ì¶”ì  ì‹œì‘
        tracker = get_cost_tracker()
        tracker.start_task("full_pipeline", description=f"ì „ì²´ íŒŒì´í”„ë¼ì¸: {query[:30]}...")

        # RAG ê²€ìƒ‰ â†’ êµìˆ˜ ì§‘ê³„ â†’ AHP ë­í‚¹ â†’ ë¦¬í¬íŠ¸ ìƒì„±
        from src.rag.query.retriever import HybridRetriever
        from src.ranking.professor_aggregator import ProfessorAggregator
        from src.ranking.ranker import ProfessorRanker
        from config.settings import RETRIEVAL_TOP_K, SIMILARITY_THRESHOLD
        from config.ahp_config import DEFAULT_TYPE_WEIGHTS

        # 1. RAG ê²€ìƒ‰ (ì™¸ë¶€ ì£¼ì… ë˜ëŠ” ë‚´ë¶€ ìƒì„±)
        print("RAG ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
        if retriever is None:
            retriever = HybridRetriever(doc_types=doc_types)
        raw_rag_results = retriever.retrieve(
            query=query,
            retrieval_top_k=retrieval_top_k or RETRIEVAL_TOP_K,
            similarity_threshold=SIMILARITY_THRESHOLD,
            mode="hybrid"
        )

        # RAG ê²°ê³¼ë¥¼ test_rag.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì—”í‹°í‹°/ê´€ê³„ ì •ë³´ ë³´ì¡´)
        rag_results = self._convert_rag_results(raw_rag_results)

        # 2. êµìˆ˜ë³„ ì§‘ê³„
        print("êµìˆ˜ë³„ ë¬¸ì„œ ì§‘ê³„ ì¤‘...")
        aggregator = ProfessorAggregator()
        professor_data = aggregator.aggregate_by_professor(
            rag_results=rag_results,
            doc_types=doc_types
        )
        
        # 3. AHP ë­í‚¹
        print("AHP ê¸°ë°˜ êµìˆ˜ ìˆœìœ„ í‰ê°€ ì¤‘...")
        ranker = ProfessorRanker()
        ranked_professors = ranker.rank_professors(professor_data, DEFAULT_TYPE_WEIGHTS)
        
        # 4. AHP ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•œ ë²ˆì˜ ì‹¤í–‰ì—ì„œ RAG/AHP/REPORT ë¡œê·¸ìš© ë™ì¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©)
        run_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        raw_kw = {
            "high_level": raw_rag_results.get("high_level_keywords", []),
            "low_level": raw_rag_results.get("low_level_keywords", []),
        }
        ahp_results = {
            "query": query,
            "keywords": normalize_keywords_if_duplicate_query(raw_kw, query),
            "timestamp": run_ts,
            "total_professors": len(ranked_professors),
            "type_weights": DEFAULT_TYPE_WEIGHTS,
            "ranked_professors": ranked_professors
        }

        # 5. ë¦¬í¬íŠ¸ ìƒì„±
        result = self.generate_report(
            ahp_results=ahp_results,
            rag_results=rag_results,
            few_shot_examples=few_shot_examples
        )
        result["timestamp"] = run_ts
        result["rag_results"] = rag_results
        result["ahp_results"] = ahp_results

        # ë¹„ìš© ì¶”ì  ì¢…ë£Œ
        cost_result = tracker.end_task()
        if cost_result:
            result["api_cost"] = cost_result

        return result
    
    def generate_report(
        self,
        ahp_results: Dict[str, Any],
        rag_results: Optional[Dict[str, Any]] = None,
        few_shot_examples: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        AHP ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            ahp_results: AHP ê²°ê³¼ JSON (ahp_results_*.json íŒŒì¼ ë‚´ìš©)
            rag_results: RAG ê²€ìƒ‰ ê²°ê³¼ (ì—”í‹°í‹°/ê´€ê³„ ì •ë³´ ì¶”ì¶œìš©, Noneì´ë©´ ahp_resultsì—ì„œ ì¶”ì¶œ ì‹œë„)
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
                - í˜•ì‹: [{"input": {...}, "output": "..."}, ...]
                - ë˜ëŠ”: {"examples": [{"input": {...}, "output": "..."}]}
                - ì˜ˆì‹œ íŒŒì¼: data/report_few_shot_examples.json
                - Noneì´ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
            
        Returns:
            ìƒì„±ëœ ë¦¬í¬íŠ¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        # ì…ë ¥ JSON ì¤€ë¹„
        input_json = self._prepare_input_json(ahp_results, rag_results)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt(input_json, few_shot_examples)
        
        # GPT-4o-mini í˜¸ì¶œ (ì†ë„: max_tokens ì œí•œ, temperature ë‚®ì¶¤)
        print("GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        max_tokens = getattr(self, "_max_tokens", None) or REPORT_MAX_TOKENS
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ êµìˆ˜ ì¶”ì²œ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë³´ê³ ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=max_tokens,
        )

        # ë¹„ìš© ì¶”ì 
        log_chat_usage(
            component="report_generation",
            model=self.model,
            response=response
        )

        report_text = response.choices[0].message.content or ""
        finish_reason = getattr(response.choices[0], "finish_reason", None) or ""
        truncated = finish_reason == "length"

        if truncated:
            print(
                "[ê²½ê³ ] ë³´ê³ ì„œê°€ ì¶œë ¥ í† í° ì œí•œì— ê±¸ë ¤ ì˜ë ¸ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                "config/settings.pyì˜ REPORT_MAX_TOKENS(í˜„ì¬ ìµœëŒ€ 16384)ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )

        # ì‚¬ìš©ì ê²€ìƒ‰ì–´ ì„¹ì…˜ì—ì„œ 1ì°¨/2ì°¨ í‚¤ì›Œë“œ ë¸”ë¡ì´ ìˆìœ¼ë©´ ì œê±° (í•´ë‹¹ ì •ë³´ëŠ” ë³´ê³ ì„œì— ë¯¸í‘œê¸°)
        report_text = self._inject_keyword_section(report_text, input_json)
        # êµìˆ˜/ë¬¸ì„œ í˜•ì‹ ë³´ì • (êµìˆ˜ ë²ˆí˜¸Â·ì´ë¦„ êµµê²Œ, ë¬¸ì„œ ë²ˆí˜¸Â·ìš”ì•½ ì¤„ ê³ ì •)
        report_text = self._normalize_report_format(report_text)

        # ê²°ê³¼ êµ¬ì¡°í™”
        report_data = {
            "query": ahp_results.get("query", ""),
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "report_text": report_text,
            "input_data": input_json,
            "model": self.model,
            "report_truncated": truncated,
        }
        
        return report_data
    
    def _convert_rag_results(self, raw_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        HybridRetriever ê²°ê³¼ë¥¼ test_rag.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
        merged_resultsë§Œ ì‚¬ìš© (ì´ë¯¸ ë¬¸ì„œë‹¹ 1ê±´, similarity_threshold ì´ìƒ).

        Args:
            raw_results: HybridRetriever.retrieve() ê²°ê³¼ (merged_results ì‚¬ìš©)

        Returns:
            test_rag.json í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        docs_dict = {}
        for r in raw_results.get('merged_results', []):
            no = str(r.get('metadata', {}).get('source_doc_id', ''))
            if not no:
                continue
            doc_type = r.get('doc_type', 'unknown')
            meta = r.get('metadata', {})

            if meta.get('name') is not None:
                match_info = {
                    "search_type": "local",
                    "similarity": r.get('similarity', 0),
                    "matched_entity": {
                        "name": meta.get('name', ''),
                        "entity_type": meta.get('entity_type', ''),
                        "description": r.get('document', '')
                    },
                    "neighbors_1hop": [
                        {
                            "name": n.get('name', ''),
                            "entity_type": n.get('entity_type', ''),
                            "relation_keywords": n.get('relation_keywords', []),
                            "relation_description": n.get('relation_description', '')
                        }
                        for n in r.get('neighbors', [])
                    ]
                }
            else:
                match_info = {
                    "search_type": "global",
                    "similarity": r.get('similarity', 0),
                    "matched_relation": {
                        "source_entity": meta.get('source_entity', ''),
                        "target_entity": meta.get('target_entity', ''),
                        "keywords": meta.get('keywords', ''),
                        "description": r.get('document', '')
                    },
                    "source_entity_info": r.get('source_entity_info'),
                    "target_entity_info": r.get('target_entity_info')
                }

            docs_dict[(no, doc_type)] = {
                "no": no,
                "data_type": doc_type,
                "matches": [match_info]
            }

        retrieved_docs = sorted(
            docs_dict.values(),
            key=lambda doc: doc['matches'][0].get('similarity', 0) if doc.get('matches') else 0,
            reverse=True
        )

        return {
            "query": raw_results.get('query', ''),
            "keywords": {
                "high_level": raw_results.get('high_level_keywords', []),
                "low_level": raw_results.get('low_level_keywords', [])
            },
            "retrieved_docs": retrieved_docs
        }

    def _prepare_input_json(
        self,
        ahp_results: Dict[str, Any],
        rag_results: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ì…ë ¥ JSON ì¤€ë¹„.

        ë³´ê³ ì„œì— í‘œì‹œë˜ëŠ” í•­ëª© ì¶œì²˜:
        - query: ì‚¬ìš©ì ê²€ìƒ‰ì–´(í‘œì‹œ). keywords(1ì°¨/2ì°¨)ëŠ” ë³´ê³ ì„œì— ë¯¸í‘œê¸°.
        - professors: AHP ranked_professors ìƒìœ„ 3ëª…
        - ê° êµìˆ˜ documents: AHP documents(patent/article/project) ìœ í˜•ë³„ ìƒìœ„ 3ê°œ, type/title/summary/yearë§Œ ì‚¬ìš©.

        Args:
            ahp_results: AHP ê²°ê³¼
            rag_results: RAG ê²°ê³¼ (í‚¤ì›Œë“œÂ·retrieved_docsìš©, í˜„ì¬ ë³´ê³ ì„œ í…œí”Œë¦¿ì—ì„œëŠ” ê°œì²´/ê´€ê³„ ë¯¸ì‚¬ìš©)

        Returns:
            ë¦¬í¬íŠ¸ ìƒì„±ìš© ì…ë ¥ JSON
        """
        query = ahp_results.get("query", "")
        keywords = normalize_keywords_if_duplicate_query(
            ahp_results.get("keywords", {}), query
        )
        high_level_keywords = keywords.get("high_level", [])
        low_level_keywords = keywords.get("low_level", [])

        # ë¬¸ì„œ ìœ í˜• â†’ í•œêµ­ì–´ í‘œê¸° (ë³´ê³ ì„œì—ì„œ ë…¼ë¬¸/íŠ¹í—ˆ/ì—°êµ¬ ê³¼ì œ ë³„ë¡œ êµ¬ë¶„Â·í‘œê¸°ìš©)
        DOC_TYPE_KO = {"article": "ë…¼ë¬¸", "patent": "íŠ¹í—ˆ", "project": "ì—°êµ¬ ê³¼ì œ"}

        # êµìˆ˜ë³„ ë¬¸ì„œ ì •ë³´ ì¤€ë¹„ (ìµœëŒ€ 3ëª…)
        professors_data = []
        ranked_professors = ahp_results.get("ranked_professors", [])[:3]
        
        for idx, prof in enumerate(ranked_professors, 1):
            prof_info = prof.get("professor_info", {})
            documents = prof.get("documents", {})
            document_scores = prof.get("document_scores", {})
            
            prof_docs = []
            
            for doc_type in ["patent", "article", "project"]:
                docs = documents.get(doc_type, [])
                doc_scores_list = document_scores.get(doc_type, [])
                score_dict = {str(ds.get("no", "")): ds.get("score", 0.0) for ds in doc_scores_list}
                docs_with_scores = [(doc, score_dict.get(str(doc.get("no", "")), 0.0)) for doc in docs]
                docs_with_scores.sort(key=lambda x: x[1], reverse=True)
                selected_docs = docs_with_scores[:3]
                type_ko = DOC_TYPE_KO.get(doc_type, doc_type)
                
                for doc, _ in selected_docs:
                    text = doc.get("text", "")
                    max_chars = REPORT_SUMMARY_MAX_CHARS if REPORT_SUMMARY_MAX_CHARS else 600
                    text_for_summary = text[:max_chars] + "..." if len(text) > max_chars else text
                    prof_docs.append({
                        "type": doc_type,
                        "type_ko": type_ko,
                        "title": doc.get("title", ""),
                        "summary": text_for_summary,
                        "year": doc.get("year", ""),
                    })
            
            professors_data.append({
                "number": idx,
                "name": prof_info.get("NM", ""),
                "department": f"{prof_info.get('COLG_NM', '')} {prof_info.get('HG_NM', '')}".strip(),
                "contact": prof_info.get("EMAIL", ""),
                "documents": prof_docs
            })
        
        input_json = {
            "query": query,
            "keywords": {
                "high_level": high_level_keywords,
                "low_level": low_level_keywords
            },
            "professors": professors_data
        }
        
        return input_json

    def _inject_keyword_section(self, report_text: str, input_json: Dict[str, Any]) -> str:
        """
        ë³´ê³ ì„œ ë³¸ë¬¸ì˜ 'ì‚¬ìš©ì ê²€ìƒ‰ì–´' ì„¹ì…˜ì—ì„œ 1ì°¨/2ì°¨Â·ì €ìˆ˜ì¤€Â·ê³ ìˆ˜ì¤€ í‚¤ì›Œë“œ ë¸”ë¡ì´ ìˆìœ¼ë©´ ì œê±°.
        (í•´ë‹¹ ì •ë³´ëŠ” ë³´ê³ ì„œì— í‘œê¸°í•˜ì§€ ì•ŠìŒ)
        """
        section_header = "### ğŸ” ì‚¬ìš©ì ê²€ìƒ‰ì–´"
        replacement_block = section_header + "\n\n"

        # "### ğŸ” ì‚¬ìš©ì ê²€ìƒ‰ì–´" (ë˜ëŠ” ì˜ˆì „ "ê²€ìƒ‰ ê°œìš”" í¬í•¨ ì œëª©) ë¶€í„° ë‹¤ìŒ "###" ë˜ëŠ” "---" ì§ì „ê¹Œì§€ë¥¼ í—¤ë”ë§Œ ë‚¨ê¸°ê³  ì¹˜í™˜
        pattern = re.compile(
            r"(### ğŸ” ì‚¬ìš©ì ê²€ìƒ‰ì–´(?: \(ê²€ìƒ‰ ê°œìš”\))?)\s*\n.*?(?=\n### |\n---|\n# |\Z)",
            re.DOTALL,
        )
        if pattern.search(report_text):
            return pattern.sub(replacement_block, report_text, count=1)
        return report_text

    def _normalize_report_format(self, report_text: str) -> str:
        """
        LLM ì¶œë ¥ì—ì„œ ìì£¼ í‹€ë¦¬ëŠ” ë³´ê³ ì„œ í˜•ì‹ì„ í›„ì²˜ë¦¬ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
        - êµìˆ˜ í—¤ë”: "3. êµ¬ì¶©ì™„ êµìˆ˜" â†’ "**3.** **êµ¬ì¶©ì™„ êµìˆ˜**"
        - ì†Œì†/ì´ë©”ì¼: "ì†Œì†:" â†’ "**ì†Œì†:**"
        - ë¬¸ì„œ ëª©ë¡: "[ìœ í˜•]" ì•„ë˜ ì œëª©ë§Œ ìˆê³  ë²ˆí˜¸ ì—†ëŠ” ì¤„ â†’ "  2. **ì œëª©** (ì—°ë„)", "ìš”ì•½:" â†’ "  - ìš”ì•½: "
        """
        if not report_text or not report_text.strip():
            return report_text

        lines = report_text.split("\n")
        out: List[str] = []
        # ë¬¸ì„œ ë¸”ë¡ ë‚´ ìƒíƒœ: 0=ë¸”ë¡ ë°–, 1=**[ë…¼ë¬¸]** ë“± ìœ í˜• ë¸”ë¡ ì•ˆ
        in_doc_block = False
        doc_num = 0
        # "### ğŸ‘¤ ì¶”ì²œ êµìˆ˜" êµ¬ê°„ì—ì„œë§Œ êµìˆ˜/ì†Œì†/ì´ë©”ì¼ ì •ê·œí™” ì ìš©
        in_professor_section = False

        i = 0
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()
            # ì„¹ì…˜ ì§„ì…: ì¶”ì²œ êµìˆ˜ êµ¬ê°„ (##### N. ì´ë¦„ êµìˆ˜ í¬í•¨í•´ë„ êµ¬ê°„ìœ¼ë¡œ ì¸ì‹)
            if "### ğŸ‘¤ ì¶”ì²œ êµìˆ˜" in line or ("##### " in line and "êµìˆ˜" in line) or ("##### **" in line and "êµìˆ˜" in line):
                in_professor_section = True
            if stripped.startswith("### ") and "ì¶”ì²œ êµìˆ˜" not in line:
                in_professor_section = False
            if line.strip() == "---" and in_professor_section:
                pass  # ìœ ì§€

            # ë¬¸ì„œ ìœ í˜• ë¸”ë¡ ì‹œì‘ (**[ë…¼ë¬¸]** / [ë…¼ë¬¸] / **íŠ¹í—ˆ** / íŠ¹í—ˆ ë“± ëª¨ë“  ë³€í˜•)
            def _is_doc_type_header(s: str) -> Optional[str]:
                for pattern in [
                    r"^\*\*\[(ë…¼ë¬¸|íŠ¹í—ˆ|ì—°êµ¬ ?ê³¼ì œ)\]\*\*$",
                    r"^\[(ë…¼ë¬¸|íŠ¹í—ˆ|ì—°êµ¬ ?ê³¼ì œ)\]$",
                    r"^\*\*(ë…¼ë¬¸|íŠ¹í—ˆ|ì—°êµ¬ ?ê³¼ì œ)\*\*$",
                    r"^(ë…¼ë¬¸|íŠ¹í—ˆ|ì—°êµ¬ ?ê³¼ì œ)$",
                ]:
                    m = re.match(pattern, s)
                    if m:
                        return m.group(1).replace(" ", "")
                return None

            doc_type = _is_doc_type_header(stripped)
            if doc_type:
                in_doc_block = True
                doc_num = 0
                # ìœ í˜• ë¸”ë¡ ì•ì— ë¹ˆ ì¤„ í•œ ì¤„ (ì´ì „ì´ ë¹„ì–´ ìˆì§€ ì•Šì„ ë•Œ)
                if out and out[-1].strip() and _is_doc_type_header(out[-1].strip()) is None:
                    out.append("")
                # í†µì¼ í‘œê¸°: **[ë…¼ë¬¸]** **[íŠ¹í—ˆ]** **[ì—°êµ¬ ê³¼ì œ]**
                label = "ì—°êµ¬ ê³¼ì œ" if doc_type == "ì—°êµ¬ê³¼ì œ" else doc_type
                out.append("**[" + label + "]**")
                i += 1
                continue

            # ë¬¸ì„œ ë¸”ë¡ ë: ë‹¤ìŒ êµìˆ˜(#####) ë˜ëŠ” ### ë˜ëŠ” ---
            if in_doc_block and (
                re.match(r"^\s*---\s*$", line)
                or stripped.startswith("### ")
                or (stripped.startswith("##### ") and "êµìˆ˜" in line)
            ):
                in_doc_block = False

            # ë¬¸ì„œ ë¸”ë¡ ì•ˆ(ë˜ëŠ” ì¶”ì²œ êµìˆ˜ êµ¬ê°„): ì œëª©/ìš”ì•½ í˜•ì‹ ë³´ì • (ë§ˆì§€ë§‰ êµìˆ˜ í•­ëª©ë„ ë™ì¼ ì ìš©)
            in_doc_region = in_doc_block or in_professor_section
            if in_doc_region:
                year_at_end = re.match(r"^(.+?)\s*\((\d{4})\)\s*$", line.strip())
                has_number_prefix = re.match(r"^\s*(\d+)\.\s+(.+)$", line.strip())
                # ë²ˆí˜¸ ì—†ì´ "ì œëª© (ì—°ë„)" ë§Œ ìˆëŠ” ê²½ìš° â†’ "  N. **ì œëª©** (ì—°ë„)"
                if year_at_end and not re.match(r"^\s*\d+\.\s+", stripped):
                    doc_num += 1
                    title_part = year_at_end.group(1).strip()
                    year_part = year_at_end.group(2)
                    if title_part.startswith("**") and title_part.endswith("**"):
                        new_line = f"  {doc_num}. {title_part} ({year_part})"
                    else:
                        new_line = f"  {doc_num}. **{title_part}** ({year_part})"
                    out.append(new_line)
                    i += 1
                    continue
                # ë²ˆí˜¸ëŠ” ìˆìœ¼ë‚˜ ë“¤ì—¬ì“°ê¸°/êµµê²Œ ëˆ„ë½ ("1. ì œëª© (ì—°ë„)") â†’ "  N. **ì œëª©** (ì—°ë„)". "N. ì´ë¦„ êµìˆ˜" ëŠ” ë¬¸ì„œê°€ ì•„ë‹ˆë¯€ë¡œ ì œì™¸
                if has_number_prefix:
                    num_str, rest = has_number_prefix.group(1), has_number_prefix.group(2).strip()
                    if rest.endswith("êµìˆ˜"):
                        # êµìˆ˜ í—¤ë” ì¤„(ì˜ˆ: 3. ì „ê´‘ê¸¸ êµìˆ˜) â†’ ë¬¸ì„œë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ì•„ë˜ êµìˆ˜ ë³´ì •ìœ¼ë¡œ ë„˜ê¹€
                        pass
                    else:
                        year_match = re.match(r"^(.+?)\s*\((\d{4})\)\s*$", rest)
                        if year_match:
                            doc_num = int(num_str)
                            title_part = year_match.group(1).strip()
                            year_part = year_match.group(2)
                            if title_part.startswith("**") and title_part.endswith("**"):
                                new_line = f"  {doc_num}. {title_part} ({year_part})"
                            else:
                                new_line = f"  {doc_num}. **{title_part}** ({year_part})"
                            out.append(new_line)
                            i += 1
                            continue
                        else:
                            doc_num += 1

            # ë¬¸ì„œ ë¸”ë¡ ë˜ëŠ” ì¶”ì²œ êµìˆ˜ êµ¬ê°„: "ìš”ì•½:" / "- ìš”ì•½:" ë“± â†’ "  - ìš”ì•½: " í˜•íƒœë¡œ í†µì¼ (ë§ˆì§€ë§‰ êµìˆ˜ í•­ëª© í¬í•¨)
            if in_doc_block or in_professor_section:
                s = line.strip()
                if s.startswith("ìš”ì•½:") and not s.startswith("  - ìš”ì•½:"):
                    rest = s[3:].strip()
                    out.append("  - ìš”ì•½: " + rest)
                    i += 1
                    continue
                if (s.startswith("- ìš”ì•½:") or s.startswith("-ìš”ì•½:")) and not line.startswith("  - "):
                    rest = s.split("ìš”ì•½:", 1)[-1].strip()
                    out.append("  - ìš”ì•½: " + rest)
                    i += 1
                    continue

            # ì¶”ì²œ êµìˆ˜ êµ¬ê°„: "##### N. ì´ë¦„ êµìˆ˜" ë˜ëŠ” "N. ì´ë¦„ êµìˆ˜" â†’ êµµê²Œ ë³´ì • (ì´ë¯¸ ** ìˆìœ¼ë©´ ìŠ¤í‚µ)
            if in_professor_section and "êµìˆ˜" in line and not stripped.startswith("**"):
                # "##### 3. ì „ê´‘ê¸¸ êµìˆ˜" í˜•íƒœ
                m_heading = re.match(r"^(#####\s+)(\d+)\.\s+(.+?)\s*êµìˆ˜\s*$", stripped)
                if m_heading:
                    prefix, num, name = m_heading.group(1), m_heading.group(2), m_heading.group(3).strip()
                    out.append(prefix + "**" + num + ".** **" + name + " êµìˆ˜**")
                    i += 1
                    continue
                # "3. ì „ê´‘ê¸¸ êµìˆ˜" ë‹¨ë… ì¤„
                m_plain = re.match(r"^(\d+)\.\s+(.+?)\s*êµìˆ˜\s*$", stripped)
                if m_plain:
                    num, name = m_plain.group(1), m_plain.group(2).strip()
                    out.append("**" + num + ".** **" + name + " êµìˆ˜**")
                    i += 1
                    continue

            # ì¶”ì²œ êµìˆ˜ êµ¬ê°„: "ì†Œì†:" / "ì´ë©”ì¼:" ì•ì— ** ì—†ìœ¼ë©´ ì¶”ê°€
            if in_professor_section:
                if re.match(r"^ì†Œì†:\s*", stripped) and not stripped.startswith("**"):
                    out.append("**ì†Œì†:** " + line.strip()[3:].strip())
                    i += 1
                    continue
                if re.match(r"^ì´ë©”ì¼:\s*", stripped) and not stripped.startswith("**"):
                    out.append("**ì´ë©”ì¼:** " + line.strip()[4:].strip())
                    i += 1
                    continue

            out.append(line)
            i += 1

        # 2ì°¨ íŒ¨ìŠ¤: [íŠ¹í—ˆ]/[ì—°êµ¬ ê³¼ì œ] ë“±ì—ì„œ ë¹ ì§„ ì œëª©Â·ìš”ì•½ í˜•ì‹ í•œ ë²ˆ ë” ë³´ì •
        result = "\n".join(out)
        return self._normalize_doc_format_second_pass(result)

    def _normalize_doc_format_second_pass(self, text: str) -> str:
        """**[ë…¼ë¬¸]** **[íŠ¹í—ˆ]** **[ì—°êµ¬ ê³¼ì œ]** ë¸”ë¡: ìœ í˜• í—¤ë” í†µì¼, ì œëª©/ìš”ì•½ ë“¤ì—¬ì“°ê¸°(2ì¹¸), ìœ í˜• ì• ë¹ˆ ì¤„."""
        if not text or not text.strip():
            return text

        def _is_doc_type_header(s: str) -> Optional[str]:
            for pattern in [
                r"^\*\*\[(ë…¼ë¬¸|íŠ¹í—ˆ|ì—°êµ¬ ?ê³¼ì œ)\]\*\*$",
                r"^\[(ë…¼ë¬¸|íŠ¹í—ˆ|ì—°êµ¬ ?ê³¼ì œ)\]$",
                r"^\*\*(ë…¼ë¬¸|íŠ¹í—ˆ|ì—°êµ¬ ?ê³¼ì œ)\*\*$",
                r"^(ë…¼ë¬¸|íŠ¹í—ˆ|ì—°êµ¬ ?ê³¼ì œ)$",
            ]:
                m = re.match(pattern, s)
                if m:
                    return m.group(1).replace(" ", "")
            return None

        lines = text.split("\n")
        out: List[str] = []
        in_doc_block = False
        doc_num = 0
        for line in lines:
            s = line.strip()
            doc_type = _is_doc_type_header(s)
            if doc_type:
                in_doc_block = True
                doc_num = 0
                # ìœ í˜• ë¸”ë¡ ì•ì— ë¹ˆ ì¤„ í•œ ì¤„ (ì´ì „ ì¤„ì´ ë¹„ì–´ ìˆì§€ ì•Šì„ ë•Œ)
                if out and out[-1].strip() and not _is_doc_type_header(out[-1].strip()):
                    out.append("")
                label = "ì—°êµ¬ ê³¼ì œ" if doc_type == "ì—°êµ¬ê³¼ì œ" else doc_type
                out.append("**[" + label + "]**")
                continue
            if in_doc_block and (
                re.match(r"^\s*---\s*$", line)
                or s.startswith("### ")
                or (s.startswith("##### ") and "êµìˆ˜" in line)
            ):
                in_doc_block = False
            if in_doc_block:
                year_m = re.match(r"^(.+?)\s*\((\d{4})\)\s*$", s)
                # ì´ë¯¸ "  2. **ì œëª©** (ì—°ë„)" í˜•íƒœë©´ ë“¤ì—¬ì“°ê¸°ë§Œ ê²€ì‚¬
                has_num_and_year = re.match(r"^\s*(\d+)\.\s+(.+)\s*\((\d{4})\)\s*$", s)
                if year_m and not has_num_and_year:
                    doc_num += 1
                    title_part = year_m.group(1).strip()
                    year_part = year_m.group(2)
                    if title_part.startswith("**") and title_part.endswith("**"):
                        out.append(f"  {doc_num}. {title_part} ({year_part})")
                    else:
                        out.append(f"  {doc_num}. **{title_part}** ({year_part})")
                    continue
                if has_num_and_year:
                    num_str, mid, year_part = has_num_and_year.group(1), has_num_and_year.group(2).strip(), has_num_and_year.group(3)
                    doc_num = int(num_str)
                    if mid.startswith("**") and mid.endswith("**"):
                        new_line = f"  {doc_num}. {mid} ({year_part})"
                    else:
                        new_line = f"  {doc_num}. **{mid}** ({year_part})"
                    out.append(new_line)
                    continue
                if s.startswith("ìš”ì•½:") and not s.startswith("  - ìš”ì•½:"):
                    rest = s[3:].strip()
                    out.append("  - ìš”ì•½: " + rest)
                    continue
                if (s.startswith("- ìš”ì•½:") or s.startswith("-ìš”ì•½:")) and not line.startswith("  - "):
                    rest = s.split("ìš”ì•½:", 1)[-1].strip()
                    out.append("  - ìš”ì•½: " + rest)
                    continue
            out.append(line)
        return "\n".join(out)

    def _build_prompt(
        self,
        input_json: Dict[str, Any],
        few_shot_examples: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        
        Args:
            input_json: ì…ë ¥ JSON ë°ì´í„°
            few_shot_examples: ë³´ê³ ì„œ ìƒì„±ìš© Few-shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
                - ê° ì˜ˆì‹œëŠ” {"input": {...}, "output": "..."} í˜•ì‹
                - ë³´ê³ ì„œ ìƒì„± í˜•ì‹ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì˜ˆì‹œ ë°ì´í„°
                - ì°¸ê³  íŒŒì¼: data/report_few_shot_examples.json
            
        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        base_prompt = """ë‹¹ì‹ ì€ ì‚°í•™í˜‘ë ¥ ë§¤ì¹­ì„ ìœ„í•œ **ê³µì‹ ì¶”ì²œ ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì…ë ¥ëœ ê²€ìƒ‰ ì§ˆì˜ì™€ ì¶”ì²œ êµìˆ˜Â·ë¬¸ì„œ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬, **Word ë¬¸ì„œì²˜ëŸ¼ êµ¬ì¡°í™”Â·ê°€ë…ì„± ë†’ì€ ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

[ì§€ì¹¨]
- ì…ë ¥ JSONì˜ ê°’ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ë¡ Â·í•´ì„Â·í‰ê°€ ë¬¸ì¥ì„ ë„£ì§€ ë§ˆì„¸ìš”.
- **ë§ˆí¬ë‹¤ìš´ í™œìš©**: ì œëª©ì€ #(ëŒ€ì œëª©), ##(ì„¹ì…˜), ###(ì†Œì œëª©)ìœ¼ë¡œ ê³„ì¸µì„ ë‚˜ëˆ„ê³ , **êµµê²Œ**ëŠ” **í‚¤ì›Œë“œ**ì²˜ëŸ¼ ë°˜ë“œì‹œ ì‚¬ìš©í•˜ì„¸ìš”.
- **ê°•ì¡°**: "ì‚¬ìš©ì ê²€ìƒ‰ì–´", **"êµìˆ˜ëª…"**, "ì†Œì†", "ì´ë©”ì¼", "ë¬¸ì„œ ìœ í˜•", "ì œëª©", "ì—°ë„" ë“± ë¼ë²¨ì€ **êµµê²Œ** ì²˜ë¦¬í•˜ì„¸ìš”. êµìˆ˜ í‘œê¸°ëŠ” **ì´ë¦„ê³¼ 'êµìˆ˜'ê¹Œì§€ í†µì§¸ë¡œ êµµê²Œ** í•˜ì„¸ìš”. ì˜ˆ: **í™ê¸¸ë™ êµìˆ˜**.
- **ì´ëª¨í‹°ì½˜**: ì„¹ì…˜ êµ¬ë¶„ì„ ìœ„í•´ ê° ì„¹ì…˜ ì œëª© ì•ì— ì´ëª¨í‹°ì½˜ì„ í•˜ë‚˜ì”© ë„£ìœ¼ì„¸ìš”. ì˜ˆ: ğŸ“‹ ì œëª©, ğŸ” ì‚¬ìš©ì ê²€ìƒ‰ì–´, ğŸ‘¤ ì¶”ì²œ êµìˆ˜, ğŸ“Œ ìœ ì˜ì‚¬í•­ ë° ë¬¸ì˜
- **ì¶”ì²œ êµìˆ˜ ìˆœì„œ**: ì¶”ì²œë˜ëŠ” êµìˆ˜ëŠ” ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ 1, 2, 3 ë²ˆí˜¸ë¥¼ ë¶™ì´ë˜, **ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸(1. 2. 3.)ë¥¼ ì“°ì§€ ë§ˆì„¸ìš”.** ê°€ë¡œì¤„(---) ë•Œë¬¸ì— ë¦¬ìŠ¤íŠ¸ê°€ ëŠê²¨ ëª¨ë‘ "1."ë¡œ ë³´ì´ëŠ” ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ, êµìˆ˜ ë²ˆí˜¸ëŠ” **êµµì€ ìˆ«ì**ë¡œë§Œ í‘œê¸°í•˜ì„¸ìš”. êµìˆ˜ í‘œê¸°ëŠ” **ì´ë¦„ + " êµìˆ˜"** ê¹Œì§€ í†µì§¸ë¡œ êµµê²Œ ì“°ì„¸ìš”. ì˜ˆ: **1.** **í™ê¸¸ë™ êµìˆ˜**, **2.** **ê¹€ì² ìˆ˜ êµìˆ˜**, **3.** **ì´ì˜í¬ êµìˆ˜**. ë‹¤ìŒ ì¤„ì— **ì†Œì†:**, **ì´ë©”ì¼:** ì€ ë¶ˆë¦¿(-) ì—†ì´ í•œ ì¤„ì”©ë§Œ í‘œê¸°í•˜ê³ , êµìˆ˜ ë¸”ë¡ ì‚¬ì´ì—ëŠ” ê°€ë¡œì¤„(---)ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
- AHP ì ìˆ˜Â·ì¢…í•© ì ìˆ˜ëŠ” ë³´ê³ ì„œì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- **ì‚¬ìš©ì ê²€ìƒ‰ì–´ ê´€ë ¨ ìë£Œ**: (1) ìœ í˜•ì€ **ëŒ€ê´„í˜¸ [ ]** ë¡œë§Œ. (2) ê° ë¬¸ì„œëŠ” ì²« ì¤„ì— `  1. **ì œëª©** (ì—°ë„)` í˜•ì‹, ë‘˜ì§¸ ì¤„ì€ **ë°˜ë“œì‹œ** `  - ìš”ì•½: ` ë¡œ ì‹œì‘(ë“¤ì—¬ì“°ê¸° 2ì¹¸ + í•˜ì´í”ˆ + ê³µë°± + ìš”ì•½:). "ìš”ì•½:"ë§Œ ë‹¨ë…ìœ¼ë¡œ ì“°ê±°ë‚˜ ë¶ˆë¦¿(-) ì—†ì´ ì“°ì§€ ë§ˆì„¸ìš”. ìš”ì•½ ë‚´ìš©ì€ 2~3ë¬¸ì¥, ì…ë ¥ JSONì˜ summaryë¥¼ ì°¸ê³ í•´ ì‚¬ìš©ì ê²€ìƒ‰ì–´ì™€ì˜ ì—°ê´€ì„±ì„ ì„¤ëª…í•˜ê³  ë¬¸ì²´ë¥¼ í†µì¼í•˜ì„¸ìš”.

---

### [ë³´ê³ ì„œ ì¶œë ¥ í˜•ì‹]

ë³´ê³ ì„œ **ë§¨ ìœ„**ì— ë‹¤ìŒ ì œëª© ë¸”ë¡ì„ ë„£ìœ¼ì„¸ìš” (ì œëª©Â·ì‚¬ìš©ì ê²€ìƒ‰ì–´ëŠ” ë³¸ë¬¸ë³´ë‹¤ í•œ ë‹¨ê³„ ì‘ê²Œ):

# ğŸ“‹ AI ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼

**ì‚¬ìš©ì ê²€ìƒ‰ì–´:** (ì…ë ¥ JSONì˜ query ê°’)
---

ê·¸ ë‹¤ìŒ ì•„ë˜ ì„¹ì…˜ì„ **ìˆœì„œëŒ€ë¡œ** ì‘ì„±í•˜ì„¸ìš”. ê° ì„¹ì…˜ ì œëª© ì•ì— ì´ëª¨í‹°ì½˜ì„ ë¶™ì´ê³ , ë¼ë²¨ì€ **êµµê²Œ** ì²˜ë¦¬í•˜ì„¸ìš”.

---

### ğŸ‘¤ ì¶”ì²œ êµìˆ˜ ë° ê´€ë ¨ ì •ë³´

professors ë°°ì—´ì„ **ìˆœì„œëŒ€ë¡œ** ì‚¬ìš©í•˜ì„¸ìš”. êµìˆ˜ ë²ˆí˜¸ëŠ” **1.** **2.** **3.** ì²˜ëŸ¼ êµµì€ ìˆ«ìë¡œë§Œ ì“°ê³ , **êµìˆ˜ í‘œê¸°ëŠ” "ì´ë¦„ êµìˆ˜" ì „ì²´ë¥¼ êµµê²Œ** í‘œê¸°í•˜ì„¸ìš”. ì˜ˆ: **1.** **í™ê¸¸ë™ êµìˆ˜**. êµìˆ˜ëª… í•œ ì¤„ ë‹¤ìŒì— ì†Œì†Â·ì´ë©”ì¼ì„ ë¶ˆë¦¿ ì—†ì´ í•œ ì¤„ì”© í‘œê¸°í•˜ê³ , êµìˆ˜ ë¸”ë¡ ì‚¬ì´ì—ëŠ” ê°€ë¡œì¤„(---)ì„ ë„£ì–´ êµ¬ë¶„í•˜ì„¸ìš”. **ì‚¬ìš©ì ê²€ìƒ‰ì–´ ê´€ë ¨ ìë£Œ** ì‘ì„± ê·œì¹™:
- **ìœ í˜•**: ëŒ€ê´„í˜¸ [ ] ë§Œ ì‚¬ìš©. **[ë…¼ë¬¸]** **[íŠ¹í—ˆ]** **[ì—°êµ¬ ê³¼ì œ]** ì¤‘ í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ í‘œê¸°.
- **ë¬¸ì„œ**: ìœ í˜• ì•„ë˜ **ë°˜ë“œì‹œ** (1) ì²« ì¤„: `  1. **ì œëª©** (ì—°ë„)` (ë“¤ì—¬ì“°ê¸° 2ì¹¸ + ë²ˆí˜¸ + **ì œëª©** + ê³µë°± + (ì—°ë„)), (2) ë‘˜ì§¸ ì¤„: `  - ìš”ì•½: ` ë¡œ ì‹œì‘í•œ ë’¤ 2~3ë¬¸ì¥. **ìš”ì•½ ì¤„ì€ ì˜ˆì™¸ ì—†ì´ ë°˜ë“œì‹œ "  - ìš”ì•½: "ìœ¼ë¡œ ì‹œì‘**í•˜ì„¸ìš”. "ìš”ì•½:"ë§Œ ë‹¨ë…ìœ¼ë¡œ ì“°ê±°ë‚˜ ë¶ˆë¦¿(-) ì—†ì´ ì“°ì§€ ë§ˆì„¸ìš”.
- **ì˜ëª»ëœ ì˜ˆ(ê¸ˆì§€)**: `ì œëª© (ì—°ë„)\nìš”ì•½: ë‚´ìš©` ë˜ëŠ” `ì œëª©\nìš”ì•½: ë‚´ìš©` â†’ ì´ë ‡ê²Œ í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ë²ˆí˜¸(1. 2. 3.)ì™€ `  - ìš”ì•½: ` í˜•ì‹ì„ ì§€í‚¤ì„¸ìš”.
- **ìœ í˜• ê°„ ì¤„ ê°„ê²©**: **[ë…¼ë¬¸]** / **[íŠ¹í—ˆ]** / **[ì—°êµ¬ ê³¼ì œ]** ë¸”ë¡ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ì„ í•œ ì¤„ ì´ìƒ ë„£ì–´ êµ¬ë¶„í•˜ì„¸ìš”.

(documents ë°°ì—´ì˜ type_ko, title, summary, year ì‚¬ìš©. ìœ í˜• ì•„ë˜ì— ë¬¸ì„œê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ìœ í˜•ì€ ìƒëµ)

##### **1.** **[ì´ë¦„] êµìˆ˜**
(ìœ„ [ì´ë¦„]ì€ ì…ë ¥ JSONì˜ professors[].name. **ì´ë¦„ êµìˆ˜** ì „ì²´ë¥¼ êµµê²Œ. ì˜ˆ: **í™ê¸¸ë™ êµìˆ˜**)
**ì†Œì†:** (department)
**ì´ë©”ì¼:** (contact, ì—†ìœ¼ë©´ "-")

**ì‚¬ìš©ì ê²€ìƒ‰ì–´ ê´€ë ¨ ìë£Œ**
**[ë…¼ë¬¸]**
  1. **ì œëª©1** (2024)
  - ìš”ì•½: (í•´ë‹¹ ë¬¸ì„œ summaryë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ê²€ìƒ‰ì–´ì™€ì˜ ì—°ê´€ì„±ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…. ë¬¸ì²´ í†µì¼.)
  2. **ì œëª©2** (2023)
  - ìš”ì•½: (ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ 2~3ë¬¸ì¥ ì„¤ëª…. ìœ„ì™€ ê°™ì´ ë°˜ë“œì‹œ "  - ìš”ì•½: "ìœ¼ë¡œ ì‹œì‘.)

**[íŠ¹í—ˆ]**
  1. **ì œëª©** (2024)
  - ìš”ì•½: (2~3ë¬¸ì¥ìœ¼ë¡œ ì—°ê´€ì„± ì„¤ëª….)

**[ì—°êµ¬ ê³¼ì œ]**
  1. **ì œëª©** (2024)
  - ìš”ì•½: (2~3ë¬¸ì¥ìœ¼ë¡œ ì—°ê´€ì„± ì„¤ëª….)

---
(2ë²ˆ, 3ë²ˆ êµìˆ˜ëŠ” **2.** **[ì´ë¦„] êµìˆ˜**, **3.** **[ì´ë¦„] êµìˆ˜**ì²˜ëŸ¼ ë²ˆí˜¸ì™€ ì´ë¦„ë§Œ ë°”ê¿” ë°˜ë³µ. "ì´ë¦„ êµìˆ˜" ì „ì²´ë¥¼ **êµµê²Œ**. ê° êµìˆ˜ ë¸”ë¡ ëì— ---ë¡œ êµ¬ë¶„)

---

### ğŸ“Œ ìœ ì˜ì‚¬í•­ ë° ë¬¸ì˜ ì•ˆë‚´

- ì¶”ì²œ ê²°ê³¼ëŠ” ì…ë ¥í•˜ì‹  ê²€ìƒ‰ì–´ì™€ ì‹œìŠ¤í…œì— ë“±ë¡ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œê³µë˜ë©°, ê²€ìƒ‰ ì¡°ê±´ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ì¶œë ¥ ìˆœì„œëŠ” êµìˆ˜ ìˆœìœ„ë‚˜ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ ì•Šìœ¼ë©°, ë³¸ ê²°ê³¼ëŠ” ì°¸ê³  ìë£Œë¡œ í™œìš©í•´ ì£¼ì‹œê¸°ë¥¼ ë°”ëë‹ˆë‹¤.

- ë³´ë‹¤ ì •í™•í•œ ì •ë³´ë‚˜ ì‚°í•™í˜‘ë ¥ ê´€ë ¨ ìƒë‹´ì´ í•„ìš”í•˜ì‹  ê²½ìš°, ì•„ë˜ ì‚°í•™í˜‘ë ¥ë‹¨ ë‹´ë‹¹ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

| **êµ¬ë¶„** | **ë‚´ìš©** |
|------|------|
| ë‹´ë‹¹ì | ê¹€OO |
| ì´ë©”ì¼ | oo@inu.ac.kr |
| ì—°ë½ì²˜ | 032-000-0000 |

---
"""
        
        # Few-shot ì˜ˆì‹œ ì¶”ê°€ (REPORT_FEW_SHOT_MAX_EXAMPLESë¡œ ê°œìˆ˜ ì œí•œ ì‹œ ì†ë„ í–¥ìƒ)
        if few_shot_examples:
            limit = REPORT_FEW_SHOT_MAX_EXAMPLES
            examples_to_use = (
                few_shot_examples[:limit]
                if limit is not None and isinstance(limit, int)
                else few_shot_examples
            )
            for i, example in enumerate(examples_to_use, 1):
                example_input = example.get("input", {})
                example_output = example.get("output", "")
                
                base_prompt += f"""
### [âœ… Few-shot ì˜ˆì‹œ {i}]

ì…ë ¥ JSON:
{json.dumps(example_input, ensure_ascii=False, indent=2)}

ì¶œë ¥ ë³´ê³ ì„œ:
{example_output}

---
"""
        
        # ìµœì¢… ì…ë ¥ JSON ì¶”ê°€
        base_prompt += f"""
### [ğŸ§¾ ìƒˆë¡­ê²Œ ì‘ì„±í•´ì•¼ í•  ë³´ê³ ì„œ ëŒ€ìƒ JSON]

ë‹¤ìŒ JSON ë°ì´í„°ë¥¼ ìœ„ì™€ ë™ì¼í•œ í˜•ì‹(ì˜ˆì‹œ 1~2 ì°¸ì¡°)ì— ë”°ë¼ êµ¬ì¡°í™”ëœ ë³´ê³ ì„œë¡œ ì‘ì„±í•˜ì„¸ìš”:

{json.dumps(input_json, ensure_ascii=False, indent=2)}
"""
        
        return base_prompt
    
    def save_json(
        self,
        report_data: Dict[str, Any],
        filename: str = None
    ) -> Path:
        """
        JSON í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì €ì¥
        
        Args:
            report_data: ë³´ê³ ì„œ ë°ì´í„°
            filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filename is None:
            timestamp = report_data.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            filename = f"report_{timestamp}.json"
        
        file_path = self.output_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        return file_path
    
    def save_text(
        self,
        report_data: Dict[str, Any],
        filename: str = None
    ) -> Path:
        """
        í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì €ì¥
        
        Args:
            report_data: ë³´ê³ ì„œ ë°ì´í„°
            filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if filename is None:
            timestamp = report_data.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            filename = f"report_{timestamp}.txt"
        
        file_path = self.output_dir / filename
        
        report_text = report_data.get("report_text", "")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        return file_path

    def save_pdf(
        self,
        report_data: Dict[str, Any],
        filename: str = None
    ) -> tuple:
        """
        PDF í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì €ì¥. Playwright(HTMLâ†’PDF) í•œ ê²½ë¡œë§Œ ì‚¬ìš©.
        
        Returns:
            (ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None, ì„±ê³µ ì—¬ë¶€)
        """
        if filename is None:
            timestamp = report_data.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            filename = f"report_{timestamp}.pdf"
        file_path = self.output_dir / filename
        report_text = report_data.get("report_text", "")
        report_html = report_data.get("report_html")
        pdf_path = self._save_pdf_html_playwright(file_path, report_text=report_text, report_html=report_html)
        return (pdf_path, pdf_path is not None)

    def _save_pdf_html_playwright(
        self,
        file_path: Path,
        report_text: str = None,
        report_html: str = None,
    ) -> Optional[Path]:
        """
        Streamlitì— ë³´ì´ëŠ” HTMLì„ ê·¸ëŒ€ë¡œ PDFë¡œ ë³€í™˜ (Playwright).
        report_htmlì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ report_textë¥¼ ë§ˆí¬ë‹¤ìš´â†’HTML ë³€í™˜.
        """
        try:
            from playwright.sync_api import sync_playwright
        except ImportError:
            return None

        # Streamlitì—ì„œ ë„˜ê¸´ HTML ìš°ì„  ì‚¬ìš© (í™”ë©´ê³¼ 100% ë™ì¼)
        if report_html and report_html.strip():
            body_html = report_html.strip()
        elif report_text and (report_text or "").strip():
            try:
                import markdown as md_lib
            except ImportError:
                return None
            text = (report_text or "").strip()
            body_html = md_lib.markdown(text, extensions=["extra", "nl2br"])
            if not body_html.strip():
                body_html = "<p>" + _escape_html(text[:5000]) + "</p>"
        else:
            return None

        # í‘œ í—¤ë”ì— ì¸ë¼ì¸ ë°°ê²½ìƒ‰ ì¶”ê°€ (ì¸ì‡„ ì‹œ CSS ë¯¸ì ìš© í™˜ê²½ ëŒ€ë¹„)
        body_html = re.sub(
            r"<th(\s[^>]*)?>",
            r'<th style="background-color:#e8eef4; border:1px solid rgba(30,58,95,0.3); padding:4px 8px;"\1>',
            body_html,
            flags=re.IGNORECASE,
        )
        # ì‚¬ìš©ì ê²€ìƒ‰ì–´ ê´€ë ¨ ìë£Œ: "(ì—°ë„):" ë˜ëŠ” "ì—°ë„:" ë¥¼ í•œ ë©ì–´ë¦¬ë¡œ ìœ ì§€, "): " ë’¤ëŠ” ë…¼ë¦¬ì  ê³µë°± (ì½œë°± ì‚¬ìš©ìœ¼ë¡œ re ì´ìŠ¤ì¼€ì´í”„ ì˜¤ë¥˜ ë°©ì§€)
        def _year_span(match):
            return '<span class="doc-year">(' + match.group(1) + '):</span>' + chr(0x00A0)
        body_html = re.sub(r"\((\d{4})\):\s+", _year_span, body_html)
        # ì™„ì „íˆ ë¹„ì–´ ìˆëŠ” pë§Œ ì œê±° (ê³µë°±ë§Œ ìˆëŠ” pëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ ëŒ€ì²´í•´ ë‹¨ë½ ê°„ê²© ìœ ì§€)
        body_html = re.sub(r"<p>\s*</p>", "<br/>", body_html, flags=re.IGNORECASE)
        body_html = re.sub(r"<p>\s*<br\s*/?>\s*</p>", "<br/>", body_html, flags=re.IGNORECASE)

        # PDFìš© HTML: ì˜ë¦¼ ë°©ì§€(overflow ìˆ¨ê¸°ì§€ ì•ŠìŒ), ì¤„ê°„ê²©Â·ì—¬ë°± ì¶•ì†Œ, í‘œÂ·ë¦¬ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ ë³´ì¥
        head = """<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>ì‚°í•™ ë§¤ì¹­ ì¶”ì²œ ë³´ê³ ì„œ</title>
<style>
  html, body { -webkit-print-color-adjust: exact !important; print-color-adjust: exact !important; }
  * { box-sizing: border-box; }
  html { width: 100%; }
  body {
    font-family: "Malgun Gothic", "Segoe UI Emoji", "Apple Color Emoji", "Apple SD Gothic Neo", sans-serif;
    font-size: 0.95rem !important;
    line-height: 1.75 !important;
    color: #1e3a5f;
    margin: 0 !important;
    padding: 0.4rem 0.6rem !important;
    width: 100%;
    max-width: 100%;
    min-width: 0;
    word-break: keep-all;
    overflow-wrap: break-word;
    -webkit-print-color-adjust: exact;
    print-color-adjust: exact;
  }
  .report-content-box {
    background: #ffffff;
    color: #1e3a5f;
    padding: 0.5rem 0.6rem !important;
    border-radius: 6px;
    border: 1px solid rgba(30, 58, 95, 0.2);
    font-size: 0.95rem !important;
    line-height: 1.75 !important;
    width: 100%;
    max-width: 100%;
    min-width: 0;
    word-break: keep-all;
    overflow-wrap: break-word;
    -webkit-print-color-adjust: exact;
    print-color-adjust: exact;
  }
  .report-content-box h1, .report-content-box h2, .report-content-box h3, .report-content-box h4, .report-content-box h5,
  .report-content-box p, .report-content-box li, .report-content-box span,
  .report-content-box td, .report-content-box strong { line-height: 1.75 !important; }
  .report-content-box h1 { font-size: 1.15rem !important; margin: 0.6em 0 0.4em !important; color: #1e3a5f; font-weight: 700; }
  .report-content-box h2 { font-size: 1.08rem !important; margin: 0.55em 0 0.35em !important; color: #1e3a5f; font-weight: 700; }
  .report-content-box h3 { font-size: 1.02rem !important; margin: 0.5em 0 0.3em !important; color: #1e3a5f; font-weight: 700; }
  .report-content-box h4 { font-size: 0.98rem !important; margin: 0.45em 0 0.1em !important; color: #1e3a5f; font-weight: 700; }
  .report-content-box h5 { font-size: 0.96rem !important; margin: 0.4em 0 0.1em !important; color: #1e3a5f; font-weight: 700; }
  .report-content-box h5 strong { font-weight: 700 !important; }
  .report-content-box h4 + ul { margin-top: 0.1rem !important; }
  .report-content-box h4 + p { margin: 0.12em 0 !important; }
  .report-content-box h4 + p + p { margin: 0.12em 0 !important; }
  .report-content-box p { margin: 0.5em 0 !important; line-height: 1.75 !important; }
  .report-content-box ul {
    list-style-type: circle;
    list-style-position: outside;
    padding-left: 1.35rem;
    margin: 0.5rem 0 !important;
    line-height: 1.75 !important;
  }
  .report-content-box ul ul {
    list-style-type: disc;
    list-style-position: outside;
    padding-left: 1.5rem;
    margin: 0.35rem 0 0.4rem 0 !important;
    margin-top: 0.3rem !important;
  }
  .report-content-box li { margin: 0.35rem 0 !important; padding-left: 0.25rem; word-break: keep-all; overflow-wrap: break-word; line-height: 1.75 !important; }
  .report-content-box li li { margin: 0.28rem 0 !important; padding-left: 0.2rem; }
  .report-content-box strong { font-weight: 700; color: #1e3a5f; }
  .report-content-box ol + p { margin-top: 0.6em !important; }
  .report-content-box hr { border: none; border-top: 1px solid rgba(30, 58, 95, 0.25); margin: 0.6em 0 !important; }
  .report-content-box ul ul li { page-break-inside: avoid; break-inside: avoid; orphans: 2; widows: 2; }
  .report-content-box table {
    border-collapse: collapse;
    table-layout: fixed;
    width: 100%;
    max-width: 100%;
    margin: 0.5em 0 !important;
    font-size: 0.88rem !important;
    line-height: 1.6 !important;
    color: #1e3a5f;
    -webkit-print-color-adjust: exact;
    print-color-adjust: exact;
  }
  .report-content-box th, .report-content-box td {
    border: 1px solid rgba(30, 58, 95, 0.3);
    padding: 4px 8px !important;
    text-align: left;
    color: #1e3a5f;
    line-height: 1.6 !important;
    word-break: keep-all;
    overflow-wrap: anywhere;
    min-width: 0;
  }
  .report-content-box th {
    background: #e8eef4 !important;
    font-weight: 600;
    -webkit-print-color-adjust: exact;
    print-color-adjust: exact;
  }
  .report-content-box tbody tr:nth-child(even) td {
    background: #f4f6f9;
    -webkit-print-color-adjust: exact;
    print-color-adjust: exact;
  }
  @page { size: A4; margin: 18mm; }
  @media print {
    html, body { -webkit-print-color-adjust: exact !important; print-color-adjust: exact !important; }
    .report-content-box th { background: #e8eef4 !important; }
    .report-content-box tbody tr:nth-child(even) td { background: #f4f6f9 !important; }
    .report-content-box ul ul li { page-break-inside: avoid !important; break-inside: avoid !important; orphans: 2 !important; widows: 2 !important; }
  }
</style>
</head>
<body>
"""
        tail = """
</body>
</html>"""
        # ë³¸ë¬¸: ì¸ë¼ì¸ ìŠ¤íƒ€ì¼ë¡œ ì¤„ê°„ê²©Â·ê¸€ì í¬ê¸° ì ìš© (ê°€ë…ì„±)
        box_inline = "line-height:1.5; font-size:0.95rem; margin:0; padding:0.5rem 0.75rem;"
        html_doc = head + "<div class=\"report-content-box\" style=\"" + box_inline + "\">" + body_html + "</div>" + tail

        try:
            # Windows: ì„œë¸Œí”„ë¡œì„¸ìŠ¤(Chromium ì‹¤í–‰)ë¥¼ ìœ„í•´ Proactor ì´ë²¤íŠ¸ ë£¨í”„ í•„ìš” (NotImplementedError ë°©ì§€)
            if sys.platform == "win32":
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
            # A4(210mm) - ì¢Œìš° ì—¬ë°± 18mm*2 = 174mm â†’ ì•½ 657px (96dpi). ì´ ë„ˆë¹„ë¡œ ë ˆì´ì•„ì›ƒí•´ PDFì—ì„œ ê¸€ì ì˜ë¦¼ ë°©ì§€.
            content_width_px = 657
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page(viewport={"width": content_width_px, "height": 900})
                page.goto("about:blank")
                page.set_content(html_doc, wait_until="load")
                page.wait_for_timeout(1500)
                page.emulate_media(media="print")
                page.wait_for_timeout(200)
                page.pdf(
                    path=str(file_path),
                    format="A4",
                    margin={"top": "18mm", "right": "18mm", "bottom": "18mm", "left": "18mm"},
                    print_background=True,
                )
                browser.close()
            return file_path if file_path.exists() else None
        except Exception as e:
            import warnings
            msg = f"Playwright PDF ì‹¤íŒ¨(HTMLâ†’PDF ë¯¸ì ìš©). í™”ë©´ê³¼ ë™ì¼í•œ PDFë¥¼ ì“°ë ¤ë©´: playwright install chromium. ì˜¤ë¥˜: {e}"
            warnings.warn(msg)
            print(msg)
            return None
